{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1e4b9df-1bf1-4dfa-b6b5-eddc23a1ecb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from landmarks.ipynb\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "import cv2\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from mediapipe import solutions\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from fastai.vision.all import show_image\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import import_ipynb\n",
    "from landmarks import *\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose\n",
    "# mp_holistic = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86c663aa-80c5-4ed6-ad08-119358c1c22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# mediapipe hand detector (make sure hand_landmarker.task is in folder)\n",
    "base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "options = vision.HandLandmarkerOptions(base_options=base_options, min_hand_detection_confidence=.01, num_hands=2)\n",
    "detector = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "def convert_to_mediapipe(path):\n",
    "    \n",
    "    ### initialize ###\n",
    "    # open video \n",
    "    vid = cv2.VideoCapture(path)\n",
    "    assert vid.isOpened(), 'Make sure that the video is in a format accepted by c2v.VideoCapture()' \n",
    "    \n",
    "    # load first frame \n",
    "    boo, image = vid.read()\n",
    "    assert boo, 'Unable to load first frame of video'\n",
    "    \n",
    "    ### loop over frames and apply mediapipe ### \n",
    "    count = 1\n",
    "    images, landmarks = [], []\n",
    "    while boo:\n",
    "        \n",
    "        # convert image to mediapipe\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "        \n",
    "        # use mediapipe to finc image\n",
    "        result = detector.detect(mp_image)\n",
    "        \n",
    "        # cv2.imwrite(\"image\"+str(count)+\".jpeg\", image)\n",
    "        # add to lists \n",
    "        images += [image ]\n",
    "        landmarks += [ result ]\n",
    "        \n",
    "        # update \n",
    "        boo, image = vid.read()\n",
    "        count += 1\n",
    "        \n",
    "    return images, landmarks \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c4cd481-408c-4bd0-a8db-9fd132eb0701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling\n",
    "\n",
    "# Encode\n",
    "E = {\" \":0, \"!\":1, \"#\":2, \"$\":3, \"%\":4, \"&\":5, \"'\":6, \"(\":7, \")\":8, \"*\":9, \"+\":10, \",\":11, \"-\":12, \".\":13, \n",
    "          \"/\":14, \"0\":15, \"1\":16, \"2\":17, \"3\":18, \"4\":19, \"5\":20, \"6\":21, \"7\":22, \"8\":23, \"9\":24, \":\":25, \";\":26,\n",
    "          \"=\":27, \"?\":28, \"@\":29, \"[\":30, \"_\":31, \"a\":32, \"b\":33, \"c\":34, \"d\":35, \"e\":36, \"f\":37, \"g\":38, \"h\":39,\n",
    "          \"i\":40, \"j\":41, \"k\":42, \"l\":43, \"m\":44, \"n\":45, \"o\":46, \"p\":47, \"q\":48, \"r\":49, \"s\":50, \"t\":51, \"u\":52, \n",
    "          \"v\":53, \"w\":54, \"x\":55, \"y\":56, \"z\":57, \"~\":58}\n",
    "# Decode\n",
    "D = {j:i for i,j in E.items()}\n",
    "\n",
    "# Encode - converts letter to number\n",
    "def Encode(x):\n",
    "    return E[x]\n",
    "\n",
    "# Decode - converts number to letter\n",
    "def Decode(x):\n",
    "    return D[x]\n",
    "\n",
    "\n",
    "# TODO: current only has right hand\n",
    "def video_labels(self):\n",
    "     \n",
    "    # load prediction model\n",
    "    M = tf.keras.models.load_model('alphabetmodel.h5')\n",
    "    X = tf.keras.Sequential([ M, tf.keras.layers.Softmax() ])\n",
    "   \n",
    "    L = []\n",
    "    for frame in self.landmarks:\n",
    "        if frame.hand_landmarks:\n",
    "            \n",
    "            # create the landmark\n",
    "            x = landmark_pb2.LandmarkList()\n",
    "            for v in frame.hand_landmarks[0]:\n",
    "                x.landmark.add( x=v.x, y=v.y, z=v.z ) \n",
    "            \n",
    "            # center the landmark\n",
    "            x = center(x)\n",
    "            \n",
    "            # extract the data \n",
    "            data = np.array([ [ Landmark_vector(x,i) for i in range(1,21) ] ])\n",
    "            #print(data.shape)\n",
    "            p = X.predict(data, verbose=0)\n",
    "    \n",
    "            # prediction (encoded as an integer)\n",
    "            pred = np.argmax(p)\n",
    "    \n",
    "            # answer\n",
    "            ans = Decode(pred)\n",
    "            \n",
    "            # add to L\n",
    "            L += [ans]\n",
    "        else:\n",
    "            L += ['?']  \n",
    "    self.labels = L\n",
    "    \n",
    "    return self.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d671dcb-0712-4664-9086-b676aade7ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "##### Video class #####\n",
    "#######################\n",
    "                \n",
    "### Video file Class\n",
    "class video_file:           \n",
    "    def __init__(self, path):\n",
    "        \n",
    "        # initialize\n",
    "        frames, landmarkers = convert_to_mediapipe(path)\n",
    "        \n",
    "        self.path = path\n",
    "        self.frames = frames \n",
    "        self.landmarks = landmarkers\n",
    "        self.total_frames = len(frames)\n",
    "        self.landmark_percentage = 0 if self.total_frames == 0 else len([ x for x in self.landmarks if x.hand_landmarks]) / self.total_frames\n",
    "        video_labels(self) # turn this off if you do not want to automatically label video\n",
    "\n",
    "    def __repr__(self): \n",
    "            a = \" path: {0}\\n Number of frames: {1}\\n Percentage of frames with landmarks: {2}\".format( self.path,\n",
    "                                                                                                        self.total_frames, \n",
    "                                                                                                        self.landmark_percentage ) \n",
    "\n",
    "            return a \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ff71eee-e5fe-4cb8-880b-d6e165b87b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MARGIN = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "    hand_landmarks_list = detection_result.hand_landmarks\n",
    "    handedness_list = detection_result.handedness\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "\n",
    "    # Loop through the detected hands to visualize.\n",
    "    for idx in range(len(hand_landmarks_list)):\n",
    "        hand_landmarks = hand_landmarks_list[idx]\n",
    "        handedness = handedness_list[idx]\n",
    "\n",
    "        # Draw the hand landmarks.\n",
    "        hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        hand_landmarks_proto.landmark.extend([\n",
    "        landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks])\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "                                                annotated_image,\n",
    "                                                hand_landmarks_proto,\n",
    "                                                solutions.hands.HAND_CONNECTIONS,\n",
    "                                                solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "                                                solutions.drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        # Get the top left corner of the detected hand's bounding box.\n",
    "        height, width, _ = annotated_image.shape\n",
    "        x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
    "        y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
    "        text_x = int(min(x_coordinates) * width)\n",
    "        text_y = int(min(y_coordinates) * height) - MARGIN\n",
    "\n",
    "        # Draw handedness (left or right hand) on the image.\n",
    "        cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n",
    "                (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
    "                FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
    "\n",
    "    return annotated_image\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6ba27d4-9222-4d8a-b449-81a55e05462a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_annotated_video(self):\n",
    "    \n",
    "    data = self.frames\n",
    "    land = self.landmarks\n",
    "    labels = self.labels\n",
    "    n = len(data)\n",
    "    \n",
    "    # show function\n",
    "    def show_frame(i):\n",
    "        \n",
    "        # annotated image and show\n",
    "        annotated_image = draw_landmarks_on_image(data[i], land[i])\n",
    "        show_image(annotated_image, figsize=(6,6), title=f'Frame: {i} of {len(data)}     Prediction: {labels[i]}')\n",
    "    \n",
    "    return show_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df9bb450-6bfe-4949-b2ef-e9a98ad21df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 11:01:24.836900: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "#path = 'IMG_0937.MOV'\n",
    "#V = video_file(path)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce4f8d04-0bb0-4a04-b09b-7aff94700298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b163aea9e93d4dfea7f9c9612c1c61e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='i', layout=Layout(width='1000px'), max=587), Output()), â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.show_annotated_video.<locals>.show_frame(i)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#f = show_annotated_video(V)\n",
    "#interact(f, i=widgets.IntSlider(min=0, max=len(V.frames)-1, step=1, value=0, layout=widgets.Layout(width='1000px')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d849e81e-7bb8-48fe-ba97-5422fad7d609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = '/Users/benjaminbreen/Desktop/test_hand.jpg'\n",
    "# model_path = \n",
    "\n",
    "# Load the input image from an image file.\n",
    "# mp_image = mp.Image.create_from_file(model_path)\n",
    "\n",
    "# create img\n",
    "# img = cv2.imread(model_path)\n",
    "\n",
    "# STEP 2: Create an HandLandmarker object.\n",
    "#base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "#options = vision.HandLandmarkerOptions(base_options=base_options, num_hands=2)\n",
    "#detector = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "# STEP 3: Load the input image.\n",
    "# image = mp.Image.create_from_file(model_path)\n",
    "#mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "\n",
    "# STEP 4: Detect hand landmarks from the input image.\n",
    "#detection_result = detector.detect(mp_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

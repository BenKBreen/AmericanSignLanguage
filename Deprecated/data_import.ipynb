{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "706bed21-ffd9-415a-a9db-8ec3ee5d118f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f71950e0-eb07-4731-972e-c51098476c64",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mipywidgets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mwidgets\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/mediapipe/__init__.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msolutions\u001b[39;00m \n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtasks\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m framework\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m gpu\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/mediapipe/tasks/python/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The MediaPipe Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"MediaPipe Tasks API.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m components\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/mediapipe/tasks/python/audio/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"MediaPipe Tasks Audio API.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio_classifier\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio_embedder\u001b[39;00m\n\u001b[1;32m     21\u001b[0m AudioClassifier \u001b[38;5;241m=\u001b[39m audio_classifier\u001b[38;5;241m.\u001b[39mAudioClassifier\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/mediapipe/tasks/python/audio/audio_classifier.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classifier_options_pb2\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio_task_running_mode \u001b[38;5;28;01mas\u001b[39;00m running_mode_module\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base_audio_task_api\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontainers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio_data \u001b[38;5;28;01mas\u001b[39;00m audio_data_module\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontainers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_result \u001b[38;5;28;01mas\u001b[39;00m classification_result_module\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/mediapipe/tasks/python/audio/core/base_audio_task_api.py:25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio_record\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio_task_running_mode \u001b[38;5;28;01mas\u001b[39;00m running_mode_module\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptional_dependencies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m doc_controls\n\u001b[1;32m     27\u001b[0m _TaskRunner \u001b[38;5;241m=\u001b[39m task_runner_module\u001b[38;5;241m.\u001b[39mTaskRunner\n\u001b[1;32m     28\u001b[0m _Packet \u001b[38;5;241m=\u001b[39m packet_module\u001b[38;5;241m.\u001b[39mPacket\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/mediapipe/tasks/python/core/optional_dependencies.py:20\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# TensorFlow isn't a dependency of mediapipe pip package. It's only\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# required in the API docgen pipeline so we'll ignore it if tensorflow is not\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# installed.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m doc_controls\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m   \u001b[38;5;66;03m# Replace the real doc_controls.do_not_generate_docs with an no-op\u001b[39;00m\n\u001b[1;32m     23\u001b[0m   doc_controls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/__init__.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/__init__.py:36\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraceback\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py:26\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m self_check\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# TODO(mdan): Cleanup antipattern: import for side effects.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Perform pre-load sanity checks in order to produce a more actionable error.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mself_check\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreload_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m   \u001b[38;5;66;03m# This import is expected to fail if there is an explicit shared object\u001b[39;00m\n\u001b[1;32m     32\u001b[0m   \u001b[38;5;66;03m# dependency (with_framework_lib=true), since we do not need RTLD_GLOBAL.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/platform/self_check.py:63\u001b[0m, in \u001b[0;36mpreload_check\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     51\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find the DLL(s) \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m. TensorFlow requires that these DLLs \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe installed in a directory that is named in your \u001b[39m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124mPATH\u001b[39m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m           \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing))\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m   \u001b[38;5;66;03m# Load a library that performs CPU feature guard checking.  Doing this here\u001b[39;00m\n\u001b[1;32m     60\u001b[0m   \u001b[38;5;66;03m# as a preload check makes it more likely that we detect any CPU feature\u001b[39;00m\n\u001b[1;32m     61\u001b[0m   \u001b[38;5;66;03m# incompatibilities before we trigger them (which would typically result in\u001b[39;00m\n\u001b[1;32m     62\u001b[0m   \u001b[38;5;66;03m# SIGILL).\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_cpu_feature_guard\n\u001b[1;32m     64\u001b[0m   _pywrap_cpu_feature_guard\u001b[38;5;241m.\u001b[39mInfoAboutUnusedCPUFeatures()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import json\n",
    "import math\n",
    "import bisect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import ipywidgets as widgets\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from fastai.vision.all import show_image\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "# functions\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistic = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004e67e3-7f55-40a1-8595-9b497cfb3009",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b5ebaa-4fef-44ee-be73-d8b2badbf093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to a single data file\n",
    "file = '33432165'\n",
    "path_dataset = '/Volumes/My Passport for Mac/asl-fingerspelling/supplemental_landmarks/' + file + '.parquet'\n",
    "path_dataset_labels = '/Volumes/My Passport for Mac/asl-fingerspelling/supplemental_metadata.csv'\n",
    "dataset = pd.read_parquet(path_dataset)\n",
    "dataset_labels = pd.read_csv(path_dataset_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ee5b15-cf37-4ddc-ad4f-6818b188ac31",
   "metadata": {},
   "source": [
    "## Video Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d32bfb7-f60a-47a7-aefe-c11aa2ac016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video ID\n",
    "VideoID = dataset.index.unique() # 1000 total videos\n",
    "\n",
    "# Select a video\n",
    "s = 4\n",
    "seq_id = VideoID[s]  \n",
    "\n",
    "# Select video data\n",
    "data = dataset.loc[ seq_id ]\n",
    "data_labels = dataset_labels.loc[ dataset_labels['sequence_id'] == seq_id ]\n",
    "phrase = data_labels.phrase.iloc[0]\n",
    "\n",
    "# head\n",
    "data_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caecc49d-32c8-48c1-af27-ef38749901ec",
   "metadata": {},
   "source": [
    "# Data Labels\n",
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f292f8-fc83-4d22-8474-8b4db81d17c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of characters present in the sentence\n",
    "def character_list(sentence):\n",
    "    ans = []\n",
    "    words = sentence.split()\n",
    "    for word in words:\n",
    "        prev = ''\n",
    "        for i in word:\n",
    "            if i in prev or not prev: \n",
    "                prev += i\n",
    "            else:\n",
    "                ans += [prev]\n",
    "                prev = i       \n",
    "        ans += [ prev ]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ce2897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns info\n",
    "Signers = dataset_labels.participant_id.unique() # All signers\n",
    "Sentences = dataset_labels.phrase.unique() # All sentences \n",
    "\n",
    "# Characters counts\n",
    "Sentences_Characters = [character_list(i) for i in Sentences] # Convert each sentence into list of characters\n",
    "Character_Counts = Counter( sum(Sentences_Characters, []) ) # Count characters across all sentences\n",
    "\n",
    "# Convert to a sorted list\n",
    "Characters = sorted(list(Character_Counts))\n",
    "\n",
    "# Encoding and decoding characters as integers\n",
    "Encode, Decode = {}, {}\n",
    "for i,x in enumerate(Characters):\n",
    "    Encode[x] = i\n",
    "    Decode[i] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d830b8-8453-4c68-a064-1c177921876f",
   "metadata": {},
   "source": [
    "## Exploritory Data Analysis\n",
    "\n",
    "In total, there are approximately 50,000 videos in the data set which are done by 72 distinct participants. For each video, the participant will spell out the letters of a sentence that is provided to them. The sentences are often reused, so there are multiple videos of different participant spelling out the same sentence. \n",
    "\n",
    "Here is a quick summary of the data:\n",
    "\n",
    "* Total Videos: 52,958\n",
    "* Total Participants: 72\n",
    "* Total Distinct Sentences: 508\n",
    "* Total Distinct Characters: 40\n",
    "\n",
    "Below is a plot for number of videos each signer performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86035237-9797-4b14-ac12-6ad10459be48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts\n",
    "signers_count = dataset_labels['participant_id'].value_counts() # Count of videos per signer\n",
    "sentences_count = dataset_labels['phrase'].value_counts() # Count of videos per sentence\n",
    "\n",
    "# plotting number of videos per signer\n",
    "ax = signers_count.plot(kind='barh', figsize=(5,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecd2846-9609-4148-8b5e-422ba78c8220",
   "metadata": {},
   "source": [
    "### Character Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b23497-6d5c-4d86-bbbe-5e734fce6ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e2add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={\"figure.figsize\":(15, 4)})\n",
    "counts_df = pd.DataFrame(data = { 'Characters' : Characters, 'Counts' : [Character_Counts[i] for i in Characters] } )\n",
    "ax = sns.barplot(x='Characters', y='Counts', data = counts_df)\n",
    "# [ i for i in Sentences if \"ff\" in i] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c7adb8-d6af-4b04-b944-2ae9dd66e6ec",
   "metadata": {},
   "source": [
    "As is common with the english language, each characters appear with a different frequency. The characters with the highest frequency are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75da976-4f20-4071-8cc3-231a8e6b8980",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df.sort_values(by=['Counts'], ascending = False).head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae9bb2c-143e-41c6-b9b6-89576b2da29e",
   "metadata": {},
   "source": [
    "The characters with the lowest frequency are divided into two categories: \n",
    "\n",
    "1. single letters (e.g. q) \n",
    "2. double letters (e.g. tt in the word \"better\")\n",
    "\n",
    "In the latter case, the signer holds the sign for a given letter while doing \"bobbing\" or \"sliding\" motion with their hand to indicate multiple occurances of the letter. The specific letters that are doubled in the data are cc, dd, ee, ff, gg, ll, mm, nn, oo, pp, rr, ss, tt, and zz. \n",
    "\n",
    "The single characters with the lowest frequencies are listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d84aa40-cd5e-4ba4-ba3c-f4731d5eba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df[ counts_df.Characters.str.len() == 1 ].sort_values(by=['Counts']).head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae2feb8-2db8-4fe5-963c-5255583a4ca9",
   "metadata": {},
   "source": [
    "The double characters with the lowest frequencies are listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac313b10-0238-4c91-8559-2f450957d492",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df[ counts_df.Characters.str.len() == 2 ].sort_values(by=['Counts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec9308f-3d98-4634-8082-9d1a2888b7cb",
   "metadata": {},
   "source": [
    "## Data \n",
    "<p> Column Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4551da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA - Exploratory data analysis\n",
    "# data.shape # (n,k): n = number of frames in video, k = (k-1 pieces of landmark data) + (1 frame data)\n",
    "\n",
    "# Landmark data\n",
    "#### Divide columns into frame number, and landmarks for face, left_hand / right_hand, and pose. \n",
    "#    Each of the landmark columns are divided into three based on the x,y,z coordinates. We list\n",
    "#    total number of xyz columns for landmarks and then the unique landmarks. Links:\n",
    "#       - Pose:  https://developers.google.com/mediapipe/solutions/vision/pose_landmarker\n",
    "#       - Hands: https://developers.google.com/mediapipe/solutions/vision/hand_landmarker/index\n",
    "################################################################################################\n",
    "C = data.columns\n",
    "frame = C[0] # 1 frame \n",
    "face = [ i for i in C if 'face' in i] # 1404 xyz columns / 468 landmarks\n",
    "left_hand = [ i for i in C if 'left_hand' in i] # 63 xyz columns / 21 landmarks \n",
    "right_hand = [ i for i in C if 'right_hand' in i] # 63 xyz columns / 21 landmarks  \n",
    "pose = [ i for i in C if 'pose' in i] # 99 xyz columns / 33 landmarks\n",
    "\n",
    "# quick look\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748c8703-a3b4-4375-90fb-d6e3fd5e783c",
   "metadata": {},
   "source": [
    "# Landmarks\n",
    "## Import functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93ca94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of landmarks per object\n",
    "number_of_landmarks = { 'face' : 468, 'left_hand' : 21, 'right_hand' : 21, 'pose' : 33 }\n",
    "\n",
    "# simple holder for our landmarks that will mimic the results that we get back from mediapipe but using our data\n",
    "class Landmarks(object):\n",
    "    pass\n",
    "\n",
    "# Returns a \"Landmark object\" with a list of landmarks for the given object and frame   \n",
    "def get_landmarks(name, data, frame):\n",
    "    \n",
    "    # assert that name is face, left_hand, right_hand, pose\n",
    "    assert name in {'face', 'left_hand', 'right_hand', 'pose'}\n",
    "    \n",
    "    # number of landmarks per object\n",
    "    n = number_of_landmarks[name]\n",
    "    \n",
    "    # loop\n",
    "    obj = landmark_pb2.NormalizedLandmarkList()\n",
    "    for i in range(n):\n",
    "        \n",
    "        # Column names in the data (dataframe) for the x,y,z coordinates of landmark i\n",
    "        coordinates = {}\n",
    "        for j in \"xyz\":\n",
    "            column_name = j + '_' + name + '_' + str(i)\n",
    "            coordinates[j] = data[ column_name ].loc[ data['frame'] == frame ]\n",
    "        \n",
    "        # coordinates\n",
    "        xi,yi,zi = coordinates['x'], coordinates['y'], coordinates['z']\n",
    "        \n",
    "        # add to Landmark\n",
    "        obj.landmark.add( x=xi, y=yi, z=zi )\n",
    "    \n",
    "    # return \n",
    "    return obj\n",
    "\n",
    "\n",
    "# Creates all landmarks \n",
    "def get_landmarks_data(data, frame):  \n",
    "    result = Landmarks()\n",
    "    result.face_landmarks = get_landmarks('face', data, frame)\n",
    "    result.pose_landmarks = get_landmarks('pose', data, frame)\n",
    "    result.left_hand_landmarks = get_landmarks('left_hand', data, frame)\n",
    "    result.right_hand_landmarks = get_landmarks('right_hand', data, frame)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a18c55-fbc3-4392-b6c9-efb8bbbdacb3",
   "metadata": {},
   "source": [
    "## Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84de0edf-6474-4633-8ee7-748eacfd1672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if a landmark is not defined\n",
    "def Landmark_is_nan(X):\n",
    "    for i in X.landmark:\n",
    "        if math.isnan(i.x) or math.isnan(i.y) or math.isnan(i.z):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# returns <x,y,z> vector for landmark n\n",
    "def Landmark_vector(X,i):\n",
    "    return np.array([ X.landmark[i].x,  X.landmark[i].y,  X.landmark[i].z])\n",
    "    \n",
    "# Makes a blank landmark of length n\n",
    "def Landmark_blank(n):\n",
    "    obj = landmark_pb2.NormalizedLandmarkList()\n",
    "    for i in range(n):\n",
    "        obj.landmark.add( x=math.nan, y=math.nan, z=math.nan )\n",
    "    return obj\n",
    "\n",
    "# Creates a blank for all landmarks\n",
    "def blank():  \n",
    "    result = Landmarks()\n",
    "    result.face_landmarks = Landmark_blank(468)\n",
    "    result.pose_landmarks = Landmark_blank(33)\n",
    "    result.left_hand_landmarks = Landmark_blank(21)\n",
    "    result.right_hand_landmarks = Landmark_blank(21)\n",
    "    return result\n",
    "    \n",
    "# Makes a landmark undefined\n",
    "def Landmark_make_nan(X):\n",
    "    obj = landmark_pb2.NormalizedLandmarkList()\n",
    "    for i in X.landmark:\n",
    "        obj.landmark.add( x=math.nan, y=math.nan, z=math.nan )\n",
    "    return obj\n",
    "\n",
    "\n",
    "# Shifts a landmark to be centered at the vector w\n",
    "def shift(X,w):\n",
    "    obj = landmark_pb2.LandmarkList()\n",
    "    a = np.array([ X.landmark[0].x , X.landmark[0].y, X.landmark[0].z ]) \n",
    "    for i in X.landmark:\n",
    "        # form vector\n",
    "        v = np.array([i.x,i.y,i.z])\n",
    "        # unshift affine shift\n",
    "        v = v - a + w\n",
    "        # set coordinates\n",
    "        obj.landmark.add( x=v[0], y=v[1], z=v[2] )       \n",
    "    return obj \n",
    "\n",
    "\n",
    "# reflects a landmark about the y-axis\n",
    "def reflect(X):\n",
    "    obj = landmark_pb2.LandmarkList()\n",
    "    for i in X.landmark:\n",
    "        obj.landmark.add( x=(.5-i.x), y=i.y, z=i.z )       \n",
    "    return obj \n",
    "\n",
    "# records whether front or back of hand\n",
    "def front(X):\n",
    "    \n",
    "    # points\n",
    "    X0 = Landmark_vector(X,0) # base\n",
    "    X5 = Landmark_vector(X,5) # left \n",
    "    X17 = Landmark_vector(X,17) # right\n",
    "    \n",
    "    # vectors \n",
    "    V1, V2 = X5 - X0, X17 - X0\n",
    "    \n",
    "    # set z to 0 \n",
    "    V1[2], V2[2] = 0,0\n",
    "    \n",
    "    # take cross product\n",
    "    n = np.cross(V1,V2)\n",
    "    \n",
    "    return True if n[2] < 0 else False  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a27d4dd-fe11-4942-826a-fac1d7743eb8",
   "metadata": {},
   "source": [
    "## Centering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b1a18e-e0da-48f5-bb1d-01e2c9daa4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# landmarks = get_landmarks_data(video_data(V), 5)\n",
    "# X = landmarks.right_hand_landmarks\n",
    "import time \n",
    "# x = time.time()\n",
    "# a = [ center( get_landmarks_data(video_data(V), i).right_hand_landmarks ) for i in V.frames]\n",
    "# print(time.time() - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c875896a-b0a6-4f6a-9003-8fb13c9dc4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# centering using a fast tensorflow implementation\n",
    "# TODO: \n",
    "# - tensors can be made directly from landmark data\n",
    "def tf_center(X):\n",
    "    \n",
    "    # if not defined\n",
    "    if Landmark_is_nan(X): return X\n",
    "    \n",
    "    # Landmarks X0, X5, X17 and corresponding vectors V5 and V17\n",
    "    X0, X5, X17 = [ tf.convert_to_tensor(Landmark_vector(X,i)) for i in [0,5,17] ]\n",
    "    V5, V17 = X5 - X0, X17 - X0\n",
    "    \n",
    "    # normal vector to the plane of the hand \n",
    "    normal = tf.linalg.cross(V5,V17)\n",
    "    \n",
    "    # orientation\n",
    "    orientation = 'front' if 0 < normal[2] else 'back'\n",
    "    \n",
    "    # Set image - The matrix B is where you want the vectors normal, V5, and V17 to land.\n",
    "    A = tf.stack([ normal, V5, V17 ], axis=1)\n",
    "    if orientation == 'front':\n",
    "        B = tf.constant([[ 0.00605257,  0.00246374, -0.01169141], \n",
    "                         [ 0.04396006, -0.13104451, -0.0048572 ], \n",
    "                         [-0.05505851, -0.10182643, -0.04996139]], dtype=tf.float64) # front\n",
    "    else: \n",
    "        B = tf.constant([[ 0.01846211, -0.00295458,  0.02261392], \n",
    "                         [ 0.12170109, -0.12008113, -0.11504634], \n",
    "                         [ 0.18375406,  0.00450712, -0.14942883]], dtype=tf.float64) # back\n",
    "    \n",
    "    # Transformation matrix ( A^-1 * B )\n",
    "    C = tf.linalg.inv(A) @ B \n",
    "    \n",
    "    # SVD + best orthogonal approximation\n",
    "    S, U, V = tf.linalg.svd(C)\n",
    "    O = U @ V\n",
    "    \n",
    "    # rescale O\n",
    "    scale = .01\n",
    "    mag = tf.norm(normal)\n",
    "    scale_value = ( scale / mag) ** (1./3) # print(scale_value)\n",
    "    O = (scale_value) * O\n",
    "    \n",
    "    \n",
    "    obj = landmark_pb2.LandmarkList()\n",
    "    for i in X.landmark:\n",
    "            \n",
    "            # form vector\n",
    "            p = tf.constant([i.x,i.y,i.z], dtype=tf.float64)\n",
    "            \n",
    "            # affine shift\n",
    "            v = p - X0\n",
    "            \n",
    "            # linear transformation\n",
    "            w = tf.linalg.matvec(O, v)\n",
    "            \n",
    "            # set coordinates\n",
    "            obj.landmark.add( x=w[0], y=w[1], z=w[2] )\n",
    "            \n",
    "    return obj\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Recenters a hand based on the outer landmarks \n",
    "def center(X):\n",
    "    \n",
    "    # if not defined\n",
    "    if Landmark_is_nan(X): return X\n",
    "\n",
    "    # Centering\n",
    "    X0 = Landmark_vector(X,0) # base\n",
    "    X1 = Landmark_vector(X,1) # left \n",
    "    X5 = Landmark_vector(X,5) # center left \n",
    "    X9 = Landmark_vector(X,9) # center\n",
    "    X10 = Landmark_vector(X,10) # center\n",
    "    X13 = Landmark_vector(X,13) # center right\n",
    "    X17 = Landmark_vector(X,17) # right\n",
    "        \n",
    "    # Basis 1\n",
    "    V1, V5, V9, V10, V13, V17, = X1-X0, X5-X0, X9-X0, X10-X0, X13-X0, X17-X0\n",
    "    normal = np.cross(V5, V17)\n",
    "\n",
    "    # Basis Matrices\n",
    "    A, B = np.array([normal,V5,V17]),    np.array([[ 0.00605257,  0.00246374, -0.01169141], [ 0.04396006, -0.13104451, -0.0048572 ], [-0.05505851, -0.10182643, -0.04996139]])\n",
    "    # A, B = np.array([V1,V5,V17]),    np.array([[ 0.06816408, -0.02943546, -0.0151578 ], [ 0.04396006, -0.13104451, -0.0048572 ], [-0.05505851, -0.10182643, -0.04996139]])\n",
    "    # A1, B1 = np.array([V9,V5,V17]),  np.array([[ 0.12737696, -0.07470322, -0.08873697], [ 0.12005465, -0.11204314, -0.07280551], [ 0.13797943,  0.01287234, -0.11509396]])\n",
    "    # A1, B1 = np.array([V5,V10,V17]),  np.array([[ 0.12170109, -0.12008113, -0.11504634], [ 0.2935304,  -0.11416346, -0.18584773], [ 0.18375406,  0.00450712, -0.14942883]])\n",
    "    A1, B1 = np.array([normal,V5,V17]),  np.array([[ 0.01846211, -0.00295458,  0.02261392], [ 0.12170109, -0.12008113, -0.11504634], [ 0.18375406,  0.00450712, -0.14942883]])\n",
    "    # A, B = np.array([V1,V2,V3]),  np.array([[ 0.04693469, -0.12667215, -0.0208972 ],[ 0.00329348, -0.12194675, -0.03340138],[-0.07567593, -0.0876869 , -0.06737784]])\n",
    "    \n",
    "    # print(A1)\n",
    "    \n",
    "    # Transformation\n",
    "    C = (np.linalg.inv(A)).dot(B)\n",
    "    if not front(X): \n",
    "        C = (np.linalg.inv(A1)).dot(B1)\n",
    "        # print('back')\n",
    "    \n",
    "    # SVD\n",
    "    U, S, V = np.linalg.svd(C, full_matrices=True)\n",
    "    C = U.dot(V)\n",
    "    \n",
    "    # print(np.linalg.det(C))\n",
    "    \n",
    "    # resize\n",
    "    scale = .01\n",
    "    mag = np.linalg.norm( np.cross(V5,V17), 2)\n",
    "    rvalue = ( scale / mag) ** (1./3) # print(rvalue)\n",
    "    D = np.diag(np.full(3,rvalue))\n",
    "    C = D.dot(C)\n",
    "    \n",
    "    # print(np.linalg.det(C), rvalue)\n",
    "    # print(C)\n",
    "    \n",
    "    # print( np.linalg.det(C), (np.linalg.norm(V5) / np.linalg.norm(V17) ), np.dot(V5,V17) / (np.linalg.norm(V5) * np.linalg.norm(V17))  )\n",
    "    \n",
    "    obj = landmark_pb2.LandmarkList()\n",
    "    for i in X.landmark:\n",
    "            \n",
    "            # form vector\n",
    "            v = np.array([i.x,i.y,i.z])\n",
    "            \n",
    "            # affine shift\n",
    "            v = v - X0\n",
    "            \n",
    "            # linear transformation\n",
    "            v = v.dot(C)\n",
    "            \n",
    "            # set coordinates\n",
    "            obj.landmark.add( x=v[0], y=v[1], z=v[2] )\n",
    "            \n",
    "    return obj "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dcf8f7-2c5a-43b8-8cb3-3817ff72f6cd",
   "metadata": {},
   "source": [
    "# Video class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75f8438-2433-4e6a-a315-4a4477f52c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video Class\n",
    "class video:           \n",
    "    def __init__(self,seq_id):\n",
    "        self.id = seq_id \n",
    "        self.label = dataset_labels.loc[ dataset_labels['sequence_id'] == seq_id  ]\n",
    "        self.phrase = self.label.phrase.iloc[0]\n",
    "        self.participant = self.label.participant_id.iloc[0]\n",
    "        self.data = dataset.loc[ seq_id ]\n",
    "        self.frames = self.data.frame\n",
    "        self.characters = character_list(self.phrase)\n",
    "    \n",
    "    def __repr__(self): \n",
    "            a = \" Sequence: {0}\\n Phrase: {1}\\n Signer: {2}\\n Frames: {3}\\n Dominant Hand: {4}\\n Percentage: {5}\".format( self.id, \n",
    "                                                                                                                          self.phrase,\n",
    "                                                                                                                          self.participant, \n",
    "                                                                                                                          len(self.frames), \n",
    "                                                                                                                          dominant_hand(self), \n",
    "                                                                                                                          percentage(self))\n",
    "\n",
    "            return a \n",
    "#######################\n",
    "### Video functions ###\n",
    "#######################\n",
    "\n",
    "def sequence(self):\n",
    "    return self.id\n",
    "\n",
    "def participant(self):\n",
    "    return self.participant\n",
    "\n",
    "def phrase(self):\n",
    "    return self.phrase\n",
    "\n",
    "def video_data(self):\n",
    "    return self.data\n",
    "\n",
    "def frames(self):\n",
    "    return self.frames\n",
    "\n",
    "def characters(self):\n",
    "    return self.characters\n",
    "\n",
    "def percentage(self):\n",
    "    return round( 100 * len( hand_frames(self) ) / len( self.frames ), 2)\n",
    "    \n",
    "\n",
    "#######################\n",
    "#### Dominant Hand ####\n",
    "#######################\n",
    "\n",
    "def dominant_hand(self):\n",
    "    if hasattr(self, 'dominant_hand'):\n",
    "        return self.dominant_hand\n",
    "    \n",
    "    # Select all frames from data\n",
    "    data = self.data\n",
    "    frames = data.frame.unique()\n",
    "    \n",
    "    # Left / right hand frames\n",
    "    Lframes = [i for i in frames if not Landmark_is_nan(get_landmarks('left_hand',  data, i))]\n",
    "    Rframes = [i for i in frames if not Landmark_is_nan(get_landmarks('right_hand', data, i))] \n",
    "    \n",
    "    # Determine dominant hand \n",
    "    if len(Lframes) < len(Rframes):\n",
    "        self.handframes = Rframes\n",
    "        self.dominant_hand = 'right'\n",
    "    else:\n",
    "        self.handframes = Lframes\n",
    "        self.dominant_hand = 'left'\n",
    "    \n",
    "    return self.dominant_hand\n",
    "\n",
    "### Dominant hand frames\n",
    "def hand_frames(self):\n",
    "    dominant_hand(self) # this assigns handframes\n",
    "    return self.handframes\n",
    "\n",
    "#######################\n",
    "### Normalized hand ###\n",
    "#######################\n",
    "\n",
    "def normal_hand_data(self):\n",
    "    \n",
    "    if hasattr(self, 'normal_hand_data'):\n",
    "        return self.normal_hand_data, self.hand_position\n",
    "    \n",
    "    # hand frames \n",
    "    data = self.data\n",
    "    Hframes = hand_frames(self)\n",
    "    Dhand = dominant_hand(self)\n",
    "    \n",
    "    # loop \n",
    "    L, P = [], [] \n",
    "    for frame in Hframes:\n",
    "\n",
    "        # Select landmarks for dominant hand\n",
    "        X = get_landmarks_data(data, frame).right_hand_landmarks if Dhand == 'right' else get_landmarks_data(data, frame).left_hand_landmarks\n",
    "\n",
    "        # First store position\n",
    "        P.append( list(Landmark_vector(X,0)) )\n",
    "\n",
    "        # center landmark\n",
    "        X = center(X)\n",
    "\n",
    "        # List\n",
    "        L.append( [ list(Landmark_vector(X,i)) for i in range(1,21) ] )\n",
    "\n",
    "    # assign to video\n",
    "    self.normal_hand_data, self.hand_position = L, P\n",
    "    \n",
    "    # return \n",
    "    return self.normal_hand_data, self.hand_position\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fdaff8-baec-4c53-82b1-d509316d6477",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = video(seq_id)\n",
    "D = video_data(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2124083-24c9-47fc-b91a-637c2a0bb3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7910a9-788c-48e5-ab46-c7bba7805afa",
   "metadata": {},
   "source": [
    "## Exploritory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac3a2a7-132e-4035-ac80-0ed0851f808d",
   "metadata": {},
   "source": [
    "We now do some basic exploritory data analysis on the 1000 sequences in the specific data file labeled '33432165'. For each sequence, we record:\n",
    "1. number of frames \n",
    "2. percent of frames with dominant hand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78320ca1-f58b-4fc2-a7cd-3e44aa975edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a78a78d-781c-4011-b10d-03d93ea3b599",
   "metadata": {},
   "source": [
    "# Labeled Data - Method 1 \n",
    "## Clustering: k-means time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4374b97f-ca11-49e5-b7cb-2286eb3a24dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "GoodR = [4, 7, 10, 14, 15, 34, 82, 90, 100, 127, 131, 193, 219, 223, 228, 238, 339, 376, 386, 401, 457, 504, 510, 525, 528, 535, 538, 548, 570, 599, 611, 638, 651, 658, 692, 711, 784, 792, 804, 822, 835, 854, 864, 924, 940]\n",
    "V = video(VideoID[924])\n",
    "print(V.phrase, k, len(V.characters)) \n",
    "# k = find_k(V)\n",
    "# Clustering(V,k)\n",
    "# TensorFlow_Label(V,M)\n",
    "Label(V,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874214c3-2db2-4b3b-8e14-1c7c19ee8226",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9f52cb-71c5-486a-af3f-373b2649b662",
   "metadata": {},
   "source": [
    "### Finding value of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eea02d-222d-413c-9a71-834c88c941dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function returns WSS score for k values from 1 to kmax\n",
    "def calculate_WSS(self):\n",
    "    \n",
    "    # data\n",
    "    points, _ = normal_hand_data(self)\n",
    "    points = np.array(points).reshape(len(points), -1) \n",
    "    kmax = len(self.frames) - 1\n",
    "\n",
    "    sse = []\n",
    "    for k in range(1, kmax+1):\n",
    "        kmeans = KMeans(n_clusters = k, n_init = 10).fit(points)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    return sse\n",
    "\n",
    "\n",
    "def calculate_sil(self):\n",
    "    \n",
    "    # data\n",
    "    points, _ = normal_hand_data(self)\n",
    "    points = np.array(points).reshape(len(points), -1) \n",
    "    kmax = len(self.frames) - 1\n",
    "\n",
    "    # dissimilarity would not be defined for a single cluster, thus, minimum number of clusters should be 2\n",
    "    sil = []\n",
    "    for k in range(2, kmax+1):\n",
    "        kmeans = KMeans(n_clusters = k, n_init = 10).fit(points)\n",
    "        labels = kmeans.labels_\n",
    "        sil.append(silhouette_score(points, labels, metric = 'euclidean'))\n",
    "    \n",
    "    return sil "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7595ed36-92e7-4830-a002-a2876cca98df",
   "metadata": {},
   "source": [
    "#### k - Weighted square error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c7a19-3bb5-41bc-b2de-d06231afa000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for k\n",
    "def find_k(V):\n",
    "    # weighted square error for k = 0 to len(V.frames) \n",
    "    kmetric = calculate_WSS(V)\n",
    "\n",
    "    # plot\n",
    "    # plt.plot(kmetricz)\n",
    "\n",
    "    # Elbow method \n",
    "    kmetricz = [ i * len(kmetric) / kmetric[0] for i in kmetric ] # rescale x and y axis to be the same increments\n",
    "    slopes = [ kmetricz[i+1] - kmetricz[i] for i in range(len(kmetricz)-1) ] # look at slopes\n",
    "    slop = [pair for pair in enumerate(slopes)] \n",
    "    slop.sort( key = lambda x : abs(x[1]+1) ) # find slope that is closest to -1\n",
    "\n",
    "    # value of k\n",
    "    k = slop[0][0]\n",
    "    \n",
    "    return k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a871a2ce-8c0a-4c7b-9eb6-15e6e27ac71b",
   "metadata": {},
   "source": [
    "#### k - silowette method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d6d8ba-3ba7-4691-8451-ad6c2085a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# silowette score for k = 0 to len(V.frames) \n",
    "silmetric = calculate_sil(V)\n",
    "\n",
    "# plot \n",
    "# plt.plot(silmetric)\n",
    "\n",
    "# value of k (max silowette score)\n",
    "a = max(silmetric[6:])\n",
    "k0 = silmetric.index(a)\n",
    "print(k0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac6f4a4-efef-4a7d-9a51-e5e07da6a502",
   "metadata": {},
   "source": [
    "## Cluster Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a7b588-32a8-47d6-916d-07ecb750018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO\n",
    "# - GridsearchCV function below\n",
    "# - Find method for picking out strongest **re-occuring** clusters in kmeans data and labeling them\n",
    "\n",
    "\n",
    "# Label the data using a k-means time series clustering algorithm\n",
    "def label_clusters(self, k, method):\n",
    "    \n",
    "    # data\n",
    "    data, _ = normal_hand_data(self)\n",
    "    data = np.array(data).reshape(len(data), -1) \n",
    "    \n",
    "    # Select Model\n",
    "    if method   == 'kmeans': model = KMeans(n_clusters = k, n_init = 10, max_iter = 20)\n",
    "    elif method == 'ts':     model = TimeSeriesKMeans(n_clusters=k, n_init = 10, max_iter=20)\n",
    "    elif method == 'dtw':    model = TimeSeriesKMeans(n_clusters=k, n_init = 10, metric=\"dtw\", max_iter=20)\n",
    "    model.fit(data)\n",
    "   \n",
    "    # return\n",
    "    return model.labels_\n",
    "\n",
    "\n",
    "\n",
    "# label refining\n",
    "def refine_clusters(C):\n",
    "    \n",
    "    # tolerance threshold\n",
    "    d = 2\n",
    "    \n",
    "    # add buffer\n",
    "    C = list(C) + ['*']\n",
    "    \n",
    "    # Relabel data with distinct increasing labels\n",
    "    t, k, prev = 0, 0, -1\n",
    "    L = []\n",
    "    for i in C:\n",
    "        if i == prev: \n",
    "            k += 1\n",
    "        else:\n",
    "            if k < d: \n",
    "                L += ['?'] * k \n",
    "            else:\n",
    "                L += [t] * k\n",
    "                t += 1\n",
    "            k = 1\n",
    "        prev = i\n",
    "    \n",
    "    # labels\n",
    "    return L\n",
    "\n",
    "\n",
    "\n",
    "# Assigning clusters\n",
    "def Clustering(self, k):\n",
    "    \n",
    "    # parameters \n",
    "    n = 10\n",
    "    method = 'kmeans'\n",
    "    \n",
    "    # run k-means labeling n time \n",
    "    clusters = defaultdict(list)\n",
    "    for j in range(n):\n",
    "        labs = label_clusters(self, k, method)\n",
    "        rlabs = refine_clusters(labs)\n",
    "        for i,x in enumerate(rlabs):\n",
    "            clusters[i] += [x]\n",
    "    \n",
    "    # average the labels assigned to each frame\n",
    "    average_clusters, CLabels = [], []\n",
    "    for i in clusters:\n",
    "        x = max(clusters[i], key=clusters[i].count) # max label assigned to entry i\n",
    "        c = clusters[i].count(x) # count of max label assigned to entry i (used for confidence)\n",
    "        average_clusters += [x]\n",
    "        CLabels += [c]\n",
    "    \n",
    "    # refine label (make sure that they are ordered)\n",
    "    rclusters = refine_clusters(average_clusters)\n",
    "    \n",
    "    # assign\n",
    "    self.clusters = rclusters\n",
    "    \n",
    "    # return \n",
    "    return self.clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdfda7e-7c7f-4372-a1d3-54f210a378e7",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980ba50a-5f4e-473d-a2ec-a94b6d42204e",
   "metadata": {},
   "source": [
    "### Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1157b605-5265-4a7c-8845-3e0ae4744bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Data\n",
    "with open(\"LabledData40.json\", 'r') as f:\n",
    "    train = json.load(f)\n",
    "\n",
    "train_x, train_y = [], []\n",
    "for seq_id in train:\n",
    "    \n",
    "    # load date\n",
    "    V = video(int(seq_id))\n",
    "    N, p = normal_hand_data(V)\n",
    "    pN = train[seq_id]\n",
    "    \n",
    "    # loop\n",
    "    for i in range(len(N)):\n",
    "        if pN[i] != '?':\n",
    "            train_x += [ N[i] ]\n",
    "            train_y += [ Encode[ pN[i] ] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed08a8f3-bb43-4f38-93e9-564c28481fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrain, Xtest, YTrain, YTest = train_test_split(train_x, train_y, test_size=0.3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5265494-2231-4ce7-b0ba-591ba8a691e6",
   "metadata": {},
   "source": [
    "### Tensorflow model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d74841-61f7-4332-aba2-5257fbd7552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "k = len( Characters )\n",
    "M = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(20, 3)),\n",
    "    tf.keras.layers.Dense(6*k, activation='relu'),\n",
    "    tf.keras.layers.Dense(k)])\n",
    "\n",
    "# Compile Model\n",
    "M.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "M.fit( XTrain, YTrain, epochs = 500, verbose=0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb899102-1894-4635-90c4-133ece4f85b8",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92246211-54ca-49a3-8eba-21293703a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss, Acctrain = M.evaluate(XTrain, YTrain, verbose=2)\n",
    "Loss, Acctest = M.evaluate(Xtest, YTest, verbose=2)\n",
    "print('\\nTrain accuracy:', Acctrain)\n",
    "print('\\nTest accuracy:', Acctest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b8d709-e097-494c-a310-b23961fcbbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.keras.Sequential([ M, tf.keras.layers.Softmax() ])\n",
    "\n",
    "# prediction (encoded as an integer)\n",
    "YPred = [ np.argmax( X.predict( np.array([i]) , verbose=0) ) for i in Xtest ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60586359-44ac-491e-be7d-d1fbf190a217",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = pd.DataFrame()\n",
    "bad['y_true'] = [Decode[i] for i in YTest]\n",
    "bad['y_pred'] = [Decode[i] for i in YPred]\n",
    "bad['incorrect'] = (bad['y_pred'] == bad['y_true'])\n",
    "\n",
    "# counts\n",
    "A = bad.groupby(bad.columns.tolist(),as_index=False).size()\n",
    "\n",
    "A.sort_values('size',ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c39d9cf-e5ab-4307-b1de-787c183debd4",
   "metadata": {},
   "source": [
    "# Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be635668-b120-424f-ad0f-ca94619ee9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precomputed_labels(self):\n",
    "    \n",
    "    # check if assigned \n",
    "    if not hasattr(self, 'precomputedlabels'):\n",
    "        \n",
    "        # precomp\n",
    "        with open(\"LabledData5.json\", 'r') as f:\n",
    "            LabledDict = json.load(f)\n",
    "        \n",
    "        # assert\n",
    "        seq_id = str(V.id)\n",
    "        if seq_id not in LabledDict: print('Labels are not precomputed')\n",
    "        \n",
    "        # assign\n",
    "        self.precomputedlabels = LabledDict[seq_id]\n",
    "            \n",
    "    return self.precomputedlabels\n",
    "    \n",
    "       \n",
    "### Predictions\n",
    "#  Methods = 'tensor_flow', 'precomputed', or 'kmeans'\n",
    "#\n",
    "def predict(self, frame):\n",
    "    \n",
    "    method = 'precomputed'\n",
    "    \n",
    "    # TensorFlow \n",
    "    if method == 'tensor_flow': \n",
    "        \n",
    "        # data\n",
    "        data, _ = normal_hand_data(V)\n",
    "        data = np.array([ data[frame] ])\n",
    "    \n",
    "        # individual prediction\n",
    "        X = tf.keras.Sequential([ M, tf.keras.layers.Softmax() ])\n",
    "        p = X.predict( data, verbose=0)\n",
    "    \n",
    "        # prediction (encoded as an integer)\n",
    "        pred = np.argmax(p)\n",
    "    \n",
    "        # answer\n",
    "        ans = Decode[pred]\n",
    "    \n",
    "    # Kmeans\n",
    "    elif method == 'kmeans':\n",
    "        \n",
    "        # shift \n",
    "        d = 0 \n",
    "        \n",
    "        # kmeans prediction\n",
    "        x = self.data_labels[frame]\n",
    "        \n",
    "        # answer\n",
    "        ans = '?' if (x == '?' or x < d) else self.characters[x-d]\n",
    "    \n",
    "    # Precomputed \n",
    "    elif method == 'precomputed':\n",
    "        \n",
    "        # precomputed labels\n",
    "        precomp = V.data_labels \n",
    "        \n",
    "        # answer\n",
    "        ans = precomp[frame]\n",
    "                                          \n",
    "    # return\n",
    "    return ans "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c5d2b-a110-42a9-aca5-18863525fdd6",
   "metadata": {},
   "source": [
    "# Labeling Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411668d5-d741-42f1-9c68-9b2964bb6e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow Labels\n",
    "# Takes as input a video V = self and model M with labeled clusters\n",
    "def TensorFlow_Label(self, M):\n",
    "    \n",
    "    # initialize \n",
    "    clumps = self.clusters\n",
    "    characters = self.characters\n",
    "    frames, _ = normal_hand_data(self)\n",
    "    \n",
    "    # Need softmax later for predictions\n",
    "    X = tf.keras.Sequential([ M, tf.keras.layers.Softmax() ])\n",
    "    \n",
    "    ### Average predictions by cluster\n",
    "    Predictions = {} \n",
    "    cluster_labels = list({i for i in clumps if i != '?'})\n",
    "    for c in cluster_labels: \n",
    "        \n",
    "        # indices + frames\n",
    "        c_indices = [ i for i,x in enumerate(clumps) if x == c ] \n",
    "        c_frames  = [ frames[i] for i in c_indices ] \n",
    "        \n",
    "        # Average prediction\n",
    "        Predictions[c] = sum( X.predict( np.array([f]), verbose=0) for f in c_frames )[0] / len(c_frames)\n",
    "    \n",
    "        \n",
    "    ### Dynamic programing\n",
    "    # initialize\n",
    "    n, m = len(characters), len(cluster_labels)\n",
    "    \n",
    "    ## score function \n",
    "    # Given a letter (e.g. 'a') and cluster label c (e.g. 0) returns a score of the letter for that clump \n",
    "    def score( letter, c ):\n",
    "        x = Encode[ letter ]\n",
    "        s = Predictions[c][x]\n",
    "        return s\n",
    "    \n",
    "    # dp array\n",
    "    dp = defaultdict( lambda : (0,[]) ) \n",
    "    for i, letter in enumerate(characters):\n",
    "        best, labs = 0, []\n",
    "        for j in range(m):\n",
    "            \n",
    "            # update best previous score\n",
    "            if best < dp[(i-1,j-1)][0]:\n",
    "                best, labs = dp[(i-1,j-1)]\n",
    "            \n",
    "            # score at ij\n",
    "            c = cluster_labels[j]\n",
    "            score_ij = best + score( letter, c )\n",
    "            labs_ij = labs + [ c ] \n",
    "            \n",
    "            # update dp\n",
    "            dp[(i,j)] = ( score_ij, labs_ij ) \n",
    "    \n",
    "    \n",
    "    # optimal score\n",
    "    m, labs = max([ dp[(n-1,j)] for j in range(m) ])\n",
    "\n",
    "    # updating labels \n",
    "    Y_Labels = list(V.clusters)\n",
    "    for i, x in enumerate(Y_Labels):\n",
    "        if x in labs:\n",
    "            Y_Labels[i] = characters[ labs.index(x) ]\n",
    "        else:\n",
    "            Y_Labels[i] = '?'\n",
    "    # assign\n",
    "    self.data_labels = Y_Labels\n",
    "    \n",
    "    # return clusters\n",
    "    return self.data_labels   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TensorFlow Labels\n",
    "# Takes as input a video V = self and model M with labeled clusters\n",
    "def Label(self, M):\n",
    "    \n",
    "    # initialize \n",
    "    characters = self.characters\n",
    "    frames, _ = normal_hand_data(self)\n",
    "    n, m = len(characters), len(frames)\n",
    "    \n",
    "    # Need softmax later for predictions\n",
    "    X = tf.keras.Sequential([ M, tf.keras.layers.Softmax() ])\n",
    "    \n",
    "    # Predictions\n",
    "    Predictions = [] \n",
    "    for x in frames: \n",
    "        Predictions += [ X.predict(np.array([x]), verbose=0)[0] ]\n",
    "    \n",
    "    ## score function \n",
    "    # Given a letter (e.g. 'a') and cluster label c (e.g. 0) returns a score of the letter for that clump \n",
    "    def score( letter, i ):\n",
    "        # print(letter,i)\n",
    "        x = Encode[ letter ]\n",
    "        s = Predictions[i][x]\n",
    "        return s\n",
    "    \n",
    "    \n",
    "    ### Prefix sums\n",
    "    D = {}\n",
    "    for c in characters:\n",
    "        \n",
    "        # initialize\n",
    "        D[c] = defaultdict(int)\n",
    "        pre, L = 0, []\n",
    "        \n",
    "        # loop\n",
    "        for j in range(m):\n",
    "            \n",
    "            # update prefix sum\n",
    "            pre += score(c,j)\n",
    "            \n",
    "            # loop over past prefix sum and compute score of i,j\n",
    "            for i,x in enumerate(L):\n",
    "                D[c][(i,j)] = (pre - x) / math.sqrt(j-i)\n",
    "            \n",
    "            # update list of prefixes\n",
    "            L += [pre]\n",
    "        \n",
    "        # normalize scores\n",
    "        maxim = max(D[c][y] for y in D[c])\n",
    "        \n",
    "        # normalize all scores\n",
    "        for x in D[c]:\n",
    "            D[c][x] /= maxim\n",
    "            \n",
    "    \n",
    "    ### Dynamic programing\n",
    "    \n",
    "    # dp array\n",
    "    dp = defaultdict( lambda : (0,[]) ) \n",
    "    for i, letter in enumerate(characters):\n",
    "        \n",
    "        # best + labels\n",
    "        best, labs = 0, []\n",
    "        \n",
    "        # loop\n",
    "        for j in range(m):\n",
    "            \n",
    "            # update best previous score\n",
    "            if best < dp[(i-1,j-1)][0]:\n",
    "                # print(best,labs)\n",
    "                best, labs = dp[(i-1,j-1)]\n",
    "            \n",
    "            # loop over k\n",
    "            for k in range(j+1,m):\n",
    "                \n",
    "                # length of interval\n",
    "                l = k-j\n",
    "                \n",
    "                # average letter score for the frames j to k\n",
    "                score_jk = best + D[letter][(j,k)]\n",
    "                #print( (pre[letter][k] - pre[letter][j]) / l , letter, j,k)\n",
    "                \n",
    "                if dp[(i,k)][0] < score_jk:\n",
    "                    \n",
    "                    # update labels\n",
    "                    labs_jk = labs + [ (j,k,letter) ]\n",
    "                    \n",
    "                    # update dp\n",
    "                    dp[(i,k)] = ( score_jk, labs_jk ) \n",
    "    \n",
    "    \n",
    "    # optimal score\n",
    "    maxim, labs = max([ dp[(n-1,j)] for j in range(m) ])\n",
    "\n",
    "    # updating labels \n",
    "    newlabels = ['?'] * m\n",
    "    for i, j, letter in labs:\n",
    "        for k in range(i,j+1):\n",
    "            newlabels[k] = letter\n",
    "                \n",
    "    # assign\n",
    "    self.data_labels = newlabels\n",
    "    \n",
    "    # return clusters\n",
    "    return newlabels \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413a5f96-9922-4a9b-a097-511f694b4fbb",
   "metadata": {},
   "source": [
    "# Plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49fad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(landmarks,image,show_pose=True,show_face_contour=True,show_face_tesselation=True,show_left_hand=True,show_right_hand=True):\n",
    "    annotated_image = image.copy()\n",
    "    results = landmarks\n",
    "    if show_face_tesselation:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            annotated_image,\n",
    "            results.face_landmarks,\n",
    "            mp_holistic.FACEMESH_TESSELATION,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp_drawing_styles\n",
    "            .get_default_face_mesh_tesselation_style())\n",
    "    if show_face_contour:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            annotated_image,\n",
    "            results.face_landmarks,\n",
    "            mp_holistic.FACEMESH_CONTOURS,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp_drawing_styles\n",
    "            .get_default_face_mesh_contours_style())\n",
    "    if show_pose:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            annotated_image,\n",
    "            results.pose_landmarks,\n",
    "            mp_holistic.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing_styles.\n",
    "            get_default_pose_landmarks_style())\n",
    "    if show_left_hand:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            annotated_image,\n",
    "            results.left_hand_landmarks,\n",
    "            mp_holistic.HAND_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing_styles\n",
    "            .get_default_hand_landmarks_style())\n",
    "    if show_right_hand:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            annotated_image,\n",
    "            results.right_hand_landmarks,\n",
    "            mp_holistic.HAND_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing_styles\n",
    "            .get_default_hand_landmarks_style())\n",
    "    return annotated_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b492b80d-0878-4c85-ba90-6160087ef014",
   "metadata": {},
   "source": [
    "## Plot: single frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a7b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_image = np.zeros((1024,1024,3),dtype=np.uint8)\n",
    "Data, frame = video_data(V), 1\n",
    "landmarks = get_landmarks_data(Data,frame)\n",
    "#show_image(draw_landmarks(landmarks,annotated_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd26f6d9-4e55-4c70-ab71-ba83f4da894a",
   "metadata": {},
   "source": [
    "## Plot: Scrolling through frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5419dd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrolling through frames\n",
    "def show(V):\n",
    "    # initialize the data from Video V\n",
    "    Data = video_data(V)\n",
    "    Frames = frames(V)\n",
    "    # retrun frame function\n",
    "    def show_frame(frame):\n",
    "        landmarks = get_landmarks_data(Data,frame)\n",
    "        show_image(draw_landmarks(landmarks,annotated_image), figsize=(7,7), title=f'frame: {frame} [{frame+1} of {len(Frames)}]')\n",
    "    return show_frame\n",
    "\n",
    "#f = show(V)\n",
    "#interact(f, frame = widgets.IntSlider( min=0, max=len(V.frames)-1, step=0, value=0, layout=widgets.Layout(width='1000px')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfa3e4b-1607-45bd-bb9d-49bb9dab4415",
   "metadata": {},
   "source": [
    "## Plot: Dominant hand (only frame that it is visible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5982c31-7ecf-48ed-a308-a0e74b00dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrolling through image frames\n",
    "# prediction_method = 'tflow' or 'kmeans'\n",
    "def show_hands(V):    \n",
    "    \n",
    "    # initialize\n",
    "    Hframes = hand_frames(V)\n",
    "    frames = V.frames\n",
    "    phrase = V.phrase\n",
    "    Data = video_data(V)\n",
    "       \n",
    "    # show function\n",
    "    def show_frame(frame):\n",
    "        # Only do frames with dominant hand\n",
    "        frame = Hframes[frame]\n",
    "\n",
    "        # Get landmark data\n",
    "        landmarks = get_landmarks_data(Data, frame)\n",
    "\n",
    "        # Remove face and pose landmarks \n",
    "        landmarks.face_landmarks = Landmark_make_nan(landmarks.face_landmarks)\n",
    "        landmarks.pose_landmarks = Landmark_make_nan(landmarks.pose_landmarks)\n",
    "\n",
    "        # set dominant hand (if left handed we reflect about the y-axis)\n",
    "        if dominant_hand == 'left': landmarks.right_hand_landmarks = reflect(landmarks.left_hand_landmarks)\n",
    "\n",
    "        # Plot original landmark and centered hand \n",
    "        landmarks.left_hand_landmarks = shift(landmarks.right_hand_landmarks, np.array([.2,.7,0]))\n",
    "        landmarks.right_hand_landmarks = shift( center( landmarks.right_hand_landmarks ), np.array([.6,.7,0] ) )\n",
    "        show_image(draw_landmarks(landmarks,annotated_image),figsize=(6,6),title=f'frame [{frame} of {len(frames)-1}] {phrase:50} Prediction: { predict(V,frame) }')\n",
    "        #show_image(draw_landmarks(landmarks,annotated_image),figsize=(6,6),title=f'frame [{frame+1} of {len(frames)}] {phrase:50} phrase: { V.phrase }')\n",
    "        # print(f'showing frame: {frames[frame]}')\n",
    "    \n",
    "    return show_frame\n",
    "\n",
    "f = show_hands(V)\n",
    "interact(f, frame=widgets.IntSlider(min=0, max=len(V.frames)-1, step=1, value=0, layout=widgets.Layout(width='1000px')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fac37f-b7fa-4dbd-a83c-88539e8d0a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change label\n",
    "def changelabel(self, i, letter):\n",
    "    self.data_labels[i] = letter\n",
    "    \n",
    "for i in range(149,150):\n",
    "    changelabel(V,i,'?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf3945c-ebc9-463b-b9de-d5aaa6368b2f",
   "metadata": {},
   "source": [
    "### Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf4161-4648-4b05-8dac-6293056a6309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle function\n",
    "def pickle(D,V):\n",
    "    seq_id = str(V.id)\n",
    "    Frames = V.frames\n",
    "    D[seq_id] = [ predict(V,i) for i in Frames ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae0c8c9-55f2-47a2-9261-8e5cef949297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE\n",
    "# with open(\"LabledData40.json\", 'w') as f:\n",
    "    # json.dump(pickles, f, indent=2) \n",
    "\n",
    "# LOAD\n",
    "# with open(\"LabledData40.json\", 'r') as f:\n",
    "    # pickles = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b04c72c-c560-4a9c-b44d-2ae009b9d59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle(pickles,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b5bef7-3047-4481-b9fb-ebab9d82950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pickles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec3c863-7f7a-4f88-9063-f58d5ce35737",
   "metadata": {},
   "source": [
    "# Dictionary Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf40d3-fda8-4fee-89b9-07e0c4980b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counts = defaultdict(int)\n",
    "for p in pickles:\n",
    "    Vp = video(int(p))\n",
    "    print(Vp.phrase)\n",
    "    for i in Vp.characters:\n",
    "        Counts[i] += 1\n",
    "\n",
    "sorted([i for i in Counts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d381107-37d6-48fb-be18-3443e0e457b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes 804 is misspelled\n",
    "GoodDictionary = [ i for i,x in enumerate(VideoID) if str(x) in pickles ]\n",
    "BadDictionary = [i for i in GoodR if i not in GoodDictionary ]\n",
    "BadDictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9e56d6-8102-46b7-a33b-c6398d4fb070",
   "metadata": {},
   "source": [
    "# Sentence Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad1651a-eb13-4cf4-b33f-46d0a0a61a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance as lev\n",
    "\n",
    "# tolerence + video\n",
    "def reconstruct_sentence(V,M):\n",
    "    \n",
    "    # initialize k \n",
    "    k = find_k(V)\n",
    "\n",
    "    # add softmax layer for predictions\n",
    "    data, _ = normal_hand_data(V)\n",
    "    X = tf.keras.Sequential([ M, tf.keras.layers.Softmax() ])\n",
    "    \n",
    "    # clustering \n",
    "    clusters = Clustering(V,k)\n",
    "    cluster_labels = sorted(list({i for i in clusters if i != '?'}))\n",
    "    \n",
    "    # predictions\n",
    "    ans = ''\n",
    "    prev = ''\n",
    "    for c in cluster_labels:\n",
    "        \n",
    "        index_c = [ i for i,x in enumerate(clusters) if x == c] \n",
    "        \n",
    "        # individual prediction\n",
    "        p = sum( X.predict( np.array([ data[i] ]), verbose=0) for i in index_c )\n",
    "    \n",
    "        # prediction (encoded as an integer)\n",
    "        pred = np.argmax(p)\n",
    "        char = Decode[pred]\n",
    "        \n",
    "        # add to ans (if not same as previous letter)\n",
    "        if char != prev: \n",
    "            ans += Decode[pred]\n",
    "        prev = char\n",
    "    \n",
    "    # match words with phrase using Levenshtein distance\n",
    "    words = V.phrase.split()\n",
    "    \n",
    "    # loop - do dp instead\n",
    "    # w = ''\n",
    "    # rans = ''\n",
    "    # for word in words:\n",
    "        # for i in ans:\n",
    "            # n = w + i\n",
    "            # if lev(w, word) < lev(n, word):\n",
    "                #rans += w + ' '\n",
    "                #w = i\n",
    "                #break \n",
    "    #rans += w\n",
    "    \n",
    "    # return \n",
    "    return ans, V.phrase\n",
    "\n",
    "\n",
    "# TensorFlow Labels\n",
    "# Takes as input a video V = self and model M with labeled clusters\n",
    "def Corg(self, M):\n",
    "    \n",
    "    # initialize \n",
    "    frames, _ = normal_hand_data(self)\n",
    "    n, m = len(Characters), len(frames)\n",
    "    \n",
    "    # Need softmax later for predictions\n",
    "    X = tf.keras.Sequential([ M, tf.keras.layers.Softmax() ])\n",
    "    \n",
    "    # Predictions\n",
    "    Predictions = [ X.predict(np.array([x]), verbose=0)[0] for x in frames ]\n",
    "    \n",
    "    ## score function \n",
    "    # Given a letter (e.g. 'a') and cluster label c (e.g. 0) returns a score of the letter for that clump \n",
    "    def score( letter, i ):\n",
    "        # print(letter,i)\n",
    "        x = Encode[ letter ]\n",
    "        s = Predictions[i][x]\n",
    "        return s\n",
    "    \n",
    "\n",
    "    ### Prefix sums\n",
    "    D = {}\n",
    "    for c in Characters:\n",
    "        \n",
    "        # initialize\n",
    "        D[c] = defaultdict(int)\n",
    "        pre, L = 0, []\n",
    "        \n",
    "        # loop\n",
    "        for j in range(m):\n",
    "            \n",
    "            # update prefix sum\n",
    "            pre += score(c,j)\n",
    "            \n",
    "            # loop over past prefix sum and compute score of i,j\n",
    "            for i,x in enumerate(L):\n",
    "                D[c][(i,j)] = (pre - x) \n",
    "            \n",
    "            # update list of prefixes\n",
    "            L += [pre]\n",
    "        \n",
    "        # normalize scores\n",
    "        # maxim = max(D[c][y] for y in D[c])\n",
    "        \n",
    "        # normalize all scores\n",
    "        # for x in D[c]:\n",
    "            # D[c][x] /= maxim\n",
    "            \n",
    "    \n",
    "    ### Dynamic programming\n",
    "    # dp array\n",
    "    dp = defaultdict( lambda : (0,[]) )\n",
    "    for i in range(m):\n",
    "        \n",
    "        # update past\n",
    "        if dp[i][0] < dp[i-1][0]:\n",
    "            dp[i] = dp[i-1]\n",
    "        \n",
    "        # update future\n",
    "        for j in range(i+2, m):\n",
    "            \n",
    "            # loop\n",
    "            for c in Characters:\n",
    "                c_score = D[c][(i+1,j)]\n",
    "                if dp[j][0] < c_score + dp[i][0]:\n",
    "                    dp[j] = ( c_score + dp[i][0], dp[i][1] + [(i,j,c)] )\n",
    "    \n",
    "    # optimal score\n",
    "    score, labs = dp[m-1]\n",
    "    \n",
    "    # new labels\n",
    "    newlabels = ['?'] * m\n",
    "    for i, j, letter in labs:\n",
    "        for k in range(i,j+1):\n",
    "            newlabels[k] = letter \n",
    "    \n",
    "    # self.data_labels = newlabels\n",
    "            \n",
    "    # sentence\n",
    "    s = ''.join(i[2] for i in labs)\n",
    "    \n",
    "    #return s, labs, D, [ Decode[np.argmax(i)] for i in Predictions]\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Basic_Sentence(self, M): \n",
    "    \n",
    "    # initialize \n",
    "    frames, _ = normal_hand_data(self)\n",
    "    \n",
    "    # Need softmax later for predictions\n",
    "    X = tf.keras.Sequential([ M, tf.keras.layers.Softmax() ])\n",
    "    \n",
    "    # letter labels\n",
    "    Predictions = [ X.predict(np.array([x]), verbose=0)[0] for x in frames ]\n",
    "    L = [ Decode[np.argmax(i)] for i in Predictions]\n",
    "    \n",
    "    # tolerance threshold\n",
    "    d = 2\n",
    "    \n",
    "    # add buffer\n",
    "    L = list(L) + ['*']\n",
    "    \n",
    "    # Relabel data with distinct increasing labels\n",
    "    c, prev = 0, '*'\n",
    "    s = ''\n",
    "    for i in L:\n",
    "        if i == prev: \n",
    "            c += 1\n",
    "        else:\n",
    "            if d <= c: \n",
    "                s += prev\n",
    "            c = 1\n",
    "        prev = i\n",
    "    \n",
    "    # labels\n",
    "    return s\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82687708-b9ee-4965-8564-b2f5a8dc8169",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,labs,D, preds = Corg(V,M)\n",
    "a, labs, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7545517-9469-40d1-b43d-f78dbed40768",
   "metadata": {},
   "outputs": [],
   "source": [
    "D['e'][(135,140)], D['b'][(122,129)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2744130c-d88a-4e6c-89cd-06d7eb097407",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Basic_Sentence(V,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b784bcdb-2233-4101-add5-a4b2ba4cfba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = reconstruct_sentence(V,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f78dc67-5c49-48db-b090-64d67cd4500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = ''.join(V.phrase.split())\n",
    "x = 'cocmunicatsthrounghemail'\n",
    "\n",
    "N = len(correct)\n",
    "D = lev(correct, a)\n",
    "print((N-D)/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033898bb-ebd7-42a9-826f-a182611d987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(self,M):\n",
    "    \n",
    "    s1 = ''.join(self.phrase.split())\n",
    "    s2 = Corg(self, M)\n",
    "    \n",
    "    N, D = len(s1), lev(s1, s2)\n",
    "    \n",
    "    return (N-D) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d3f2a-3cf9-4ac5-86df-0987cfdbcf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = 0\n",
    "for i,x in enumerate(BadDictionary):\n",
    "    V = video(VideoID[x])\n",
    "    t = metric(V,M)\n",
    "    avg += t\n",
    "    print( V.phrase, Basic_Sentence(V,M), t, avg / (i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a279f218-0214-470f-8673-0ffb0b24d828",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "# Scrolling through image frames\n",
    "def show_frame(frame):\n",
    "    frame = Hframes[frame]\n",
    "    landmarks = get_landmarks_data(frame)\n",
    "    landmarks.face_landmarks = Landmark_make_nan(landmarks.face_landmarks)\n",
    "    landmarks.pose_landmarks = Landmark_make_nan(landmarks.pose_landmarks)\n",
    "    if dominant_hand == 'left':\n",
    "        landmarks.right_hand_landmarks = reflect(landmarks.left_hand_landmarks)\n",
    "        landmarks.left_hand_landmarks = Landmark_make_nan(landmarks.left_hand_landmarks)\n",
    "    landmarks.right_hand_landmarks = shift( center( landmarks.right_hand_landmarks, frame), np.array([.5,.7,0]))\n",
    "    # show_image(draw_landmarks(landmarks,annotated_image),figsize=(9,9),title=f'Tflow Prediction: {predict(frame):10} Actual: {prediction(frame):10} frame: {frames[frame]} [{frame+1} of {len(frames)}]')\n",
    "    show_image(draw_landmarks(landmarks,annotated_image),figsize=(9,9),title=f'Tflow Prediction: {predict(frame):10} {phrase:30} frame: {frames[frame]} [{frame+1} of {len(frames)}]')\n",
    "    #print(f'showing frame: {frames[frame]}')\n",
    "\n",
    "\n",
    "interact(show_frame, frame=widgets.IntSlider(min=0, max=len(Hframes)-1, step=1, value=0, layout=widgets.Layout(width='1000px')))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56003c49-4d71-4a47-b0c3-25d605af1d76",
   "metadata": {},
   "source": [
    "# Scripts for computing data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f13200-e089-4556-8039-020b516faa2d",
   "metadata": {},
   "source": [
    "# Tensorflow labeled alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ce637f-6c32-4ca2-8b6b-4d75f8a1779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to alphabet landmark data\n",
    "path = '/Volumes/My Passport for Mac/ASL-labled-data/archive/asl_alphabet_train/Landmarkdata.json'\n",
    "\n",
    "# open data as pickles\n",
    "with open(path, 'r') as f:\n",
    "    train_alphabet = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a85c94-2670-464f-940c-16f38718adda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateLandmark(c):\n",
    "    \n",
    "    # create a blank landmark\n",
    "    X = blank()\n",
    "    \n",
    "    # create new landmark for left hand \n",
    "    obj = landmark_pb2.LandmarkList()\n",
    "    \n",
    "    # fill in new landmark\n",
    "    for v in c: obj.landmark.add( x=v[0], y=v[1], z=v[2] ) # add other landmarks\n",
    "    \n",
    "    # set left hand to be obj\n",
    "    X.left_hand_landmarks = obj\n",
    "    X.left_hand_landmarks = center(X.left_hand_landmarks)\n",
    "    # X.left_hand_landmarks = shift( X.left_hand_landmarks, np.array([.5,.7,0]))\n",
    "    \n",
    "    return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c7a6a6-121f-4d0b-9822-dd6822a7f60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_image = np.zeros((1024,1024,3),dtype=np.uint8)\n",
    "landmarks = CreateLandmark(train_alphabet['A'][0])\n",
    "landmarks.left_hand_landmarks = shift( landmarks.left_hand_landmarks, np.array([.5,.7,0]))\n",
    "show_image(draw_landmarks(landmarks,annotated_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4e89da-e384-43e5-b800-20a162ebdf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrolling through frames\n",
    "letter = 'N'\n",
    "Frames = [ x for x in train_alphabet[letter] if x ]\n",
    "def show(frame):\n",
    "    # return frame function\n",
    "    landmarks = CreateLandmark(Frames[frame])\n",
    "    landmarks.left_hand_landmarks = shift( landmarks.left_hand_landmarks, np.array([.5,.7,0]))\n",
    "    show_image(draw_landmarks(landmarks,annotated_image), figsize=(7,7), title=f'frame: {frame}')\n",
    "\n",
    "\n",
    "interact(show, frame = widgets.IntSlider( min=0, max=len(Frames)-1, step=0, value=0, layout=widgets.Layout(width='1000px')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9811349f-5437-4c60-afd9-5ead3b9b5cd8",
   "metadata": {},
   "source": [
    "# storing nice values for x,y,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60333660-38d0-414a-b622-8c4977ae8e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "X_train, y_train = [], []\n",
    "for letter in alphabet:\n",
    "    Frames = [ x for x in train_alphabet[letter] if x ]\n",
    "    for x in Frames:\n",
    "        landmarks = CreateLandmark(x)\n",
    "        L = [ [i.x, i.y, i.z ] for i in landmarks.left_hand_landmarks.landmark ]\n",
    "        X_train += [ L[1:] ]\n",
    "        y_train += [ letter ]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d97715-857d-41bc-9a5d-f527f4ac3487",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [ Encode[i.lower()] for i in y_train ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3bfe9d-a664-4123-aed0-8036fa2003c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "k = len( Characters )\n",
    "M1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(20, 3)),\n",
    "    tf.keras.layers.Dense(6*k, activation='relu'),\n",
    "    tf.keras.layers.Dense(k)])\n",
    "\n",
    "# Compile Model\n",
    "M1.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "M1.fit( XTrain , YTrain, epochs = 20, verbose=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ff94a0-9195-43c8-8e7e-dc38f4450cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrain, Xtest, YTrain, YTest = train_test_split(X_train, y_train, test_size=0.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14891bd-7ee5-4c79-84e8-017960d85b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss, Acctrain = M1.evaluate(XTrain, YTrain, verbose=2)\n",
    "Loss, Acctest = M1.evaluate(Xtest, YTest, verbose=2)\n",
    "print('\\nTrain accuracy:', Acctrain)\n",
    "print('\\nTest accuracy:', Acctest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a326461-2f95-48ba-a286-f41db5fe8a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = [ train_x, train_y ]\n",
    "# with open(\"LabeledAlphabetData.json\", 'w') as f:\n",
    "    # indent=2 is not needed but makes the file human-readable \n",
    "    # if the data is nested\n",
    "    # json.dump(train, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ae5dbe-73a8-444c-9faa-b0eecc905bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "file = '33432165'\n",
    "frame = 47\n",
    "s = Video_ID[4] \n",
    "annotated_image = np.zeros((1024,1024,3),dtype=np.uint8)\n",
    "landmarks = get_landmarks_data(frame)\n",
    "show_image(draw_landmarks(landmarks,annotated_image))\n",
    "X = fix_z(landmarks.right_hand_landmarks)\n",
    "\n",
    "# Centering\n",
    "X0 = Landmark_vector(X,0) # base\n",
    "X1 = Landmark_vector(X,1) # left \n",
    "X5 = Landmark_vector(X,5) # center left \n",
    "X9 = Landmark_vector(X,9) # center\n",
    "X13 = Landmark_vector(X,13) # center right\n",
    "X17 = Landmark_vector(X,17) # right\n",
    "\n",
    "# correct z\n",
    "# X2[2] = predict_z( X2, Landmark_vector(X,10), Landmark_vector(X,11), Landmark_vector(X,12) )\n",
    "# X2[2] = X.landmark[10].z  \n",
    "    \n",
    "# Basis 1\n",
    "V1, V5, V9, V13, V17 = X1-X0, X5-X0, X9-X0, X13-X0, X17-X0\n",
    "\n",
    "print( V1, V5, V9, V13, V17 ) \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab8d73a-d0dd-47c1-950c-104242d52ceb",
   "metadata": {},
   "source": [
    "# Collecting complete videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defdce26-37ce-49dc-bdec-58a3d29bef8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select video data\n",
    "'''\n",
    "GoodL, GoodR = [],[]\n",
    "for j,x in enumerate(Video_ID):\n",
    "    \n",
    "    data = dataset.loc[x]\n",
    "    data_labels = dataset_labels.loc[dataset_labels['sequence_id'] == x ]\n",
    "    \n",
    "    # All frames\n",
    "    frames = data.frame.unique()\n",
    "\n",
    "    # Right / Left hand frames\n",
    "    Lframes = [i for i in frames if not Landmark_is_nan(get_landmarks('left_hand',i))]\n",
    "    Rframes = [i for i in frames if not Landmark_is_nan(get_landmarks('right_hand',i))]\n",
    "    \n",
    "    # Dominant hand \n",
    "    if len(Lframes) < len(Rframes):\n",
    "        Hframes = Rframes\n",
    "        dominant_hand = \"right\"\n",
    "    else:\n",
    "        Hframes = Lframes\n",
    "        dominant_hand = \"left\"\n",
    "    \n",
    "    print(\"Signer in %sth sequence is %s-handed and we have %s hand frames which comprise %s%% of the frames\" % (j, dominant_hand, len(Hframes), round(100 * len(Hframes) / len(frames), 2)))\n",
    "    if len(Hframes) == len(frames):\n",
    "          if dominant_hand == 'right': GoodR += [j]\n",
    "          else: GoodL += [j]\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d71665d-ff80-4605-94e1-3b2a8b6096ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vids = [ video(VideoID[i]) for i in GoodR ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f22205-e5f7-4405-8be3-1284b0fa965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[print(i) for i in Vids]\n",
    "Goodchars = set(sum([[j[0] for j in i.characters] for i in Vids], []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2592f1b9-3580-4bec-8058-e15cf47a33fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "GoodFrames = [ normal_hand_data(i) for i in Vids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e324652f-c3fb-4c8e-8408-c11aaec2401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "F = sum([i[0] for i in GoodFrames], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2328fc9-6ad4-4834-baf7-e1c89cc1f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = len(Goodchars)\n",
    "model = KMeans(n_clusters = k, n_init = 10, max_iter = 100)\n",
    "data = np.array(F).reshape(len(F), -1) \n",
    "model.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436bfb08-b3cc-4622-b69f-32848481834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = [i.reshape(20,3) for i in model.cluster_centers_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3230585-3d92-490c-92ef-67e40e8e46b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811072a8-7328-450c-8fc3-588bbb8d2e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def landmark_from_hand_data(c):\n",
    "    \n",
    "    # create a blank landmark\n",
    "    X = blank()\n",
    "    \n",
    "    # create new landmark for left hand \n",
    "    obj = landmark_pb2.LandmarkList()\n",
    "    \n",
    "    # fill in new landmark\n",
    "    obj.landmark.add( x=0, y=0, z=0 ) # origin is wrist\n",
    "    for v in c: obj.landmark.add( x=v[0], y=v[1], z=v[2] ) # add other landmarks\n",
    "    \n",
    "    # set left hand to be obj\n",
    "    X.left_hand_landmarks = obj\n",
    "    X.left_hand_landmarks = shift(X.left_hand_landmarks, np.array([.5,.7,0]))\n",
    "    \n",
    "    return X   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11fd914-8ec8-4b8e-871d-303174b6f3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_image = np.zeros((1024,1024,3),dtype=np.uint8)\n",
    "landmarks = landmark_from_hand_data(centroids[14])\n",
    "# show_image(draw_landmarks(landmarks,annotated_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d182f4-a052-49e4-9f72-b884e415e82b",
   "metadata": {},
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d468e51d-0a6d-42f7-a5fd-76c735d1c1d2",
   "metadata": {},
   "source": [
    "## Old Labeling method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726672b8-cde3-43e7-be53-94400cce4c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def label_data(self):\n",
    "    \n",
    "    # check\n",
    "    # if hasattr(self, 'data_labels'): return self.data_labels\n",
    "           \n",
    "    # list of characters\n",
    "    chars = characters(self)\n",
    "    \n",
    "    # data\n",
    "    data, _ = normal_hand_data(self)\n",
    "    data = np.array(data).reshape(len(data), -1) \n",
    "    \n",
    "    # Phase 1: Set up time series k-means algorithm for distinct symbols\n",
    "    k = len(set(i[0] for i in chars))\n",
    "    # model = TimeSeriesKMeans(n_clusters=k, metric=\"dtw\", max_iter=20)\n",
    "    # model.fit(data)\n",
    "    # Alternate Phase 1\n",
    "    model = KMeans(n_clusters = k, n_init = 10, max_iter = 20)\n",
    "    model.fit(data)\n",
    "    print(model.labels_)\n",
    "    \n",
    "    # one hot labels and run again\n",
    "    data_labels = np.array([ i for i in model.labels_ ]).reshape(-1,1)\n",
    "    \n",
    "    # Phase 2: second round of clustering based on the sentence \n",
    "    k = len(chars)\n",
    "    Model = TimeSeriesKMeans(n_clusters = k, metric = \"dtw\", max_iter = 20)\n",
    "    Model.fit(data_labels)\n",
    "    \n",
    "    # return as np.array\n",
    "    self.data_labels = Model.labels_\n",
    "   \n",
    "    # return\n",
    "    return self.data_labels\n",
    "'''   \n",
    "\n",
    "'''\n",
    "# refines the clustering data to make more uniform predictions\n",
    "def refine_labels(C, d = 4, k = 1):\n",
    "    \n",
    "    # Relabel data with distinct increasing labels\n",
    "    t, prev = 0, C[0]\n",
    "    L = []\n",
    "    for i in C:\n",
    "        if i != prev:\n",
    "            t += 1\n",
    "            prev = i\n",
    "        L += [t]\n",
    "    \n",
    "    # Count the occurences of each label\n",
    "    Counts = Counter(L)\n",
    "    # print(len(Counts))\n",
    "    \n",
    "    # loop and replace occurences with less than 4 with -1\n",
    "    temp_labels = []\n",
    "    for i in L:\n",
    "        x = i if (d <= Counts[i]) else -1   \n",
    "        temp_labels += [ x ]\n",
    "    \n",
    "    Counts = Counter(temp_labels)\n",
    "    print(len(Counts))\n",
    "    \n",
    "    # Look for clumps in the -1s\n",
    "    #A, L = [], []\n",
    "    #for i,x in enumerate(temp_labels):\n",
    "        #if x == -1: L += [i]\n",
    "        #elif L: \n",
    "            #A += [L]\n",
    "            #L = []\n",
    "            \n",
    "    # Longest sets of -1   \n",
    "    A.sort(key=len)\n",
    "    \n",
    "    # Match m and n\n",
    "    #n = len(V.characters) + 1\n",
    "    #m = len(set(temp_labels))\n",
    "    #while A and m < n:\n",
    "        #t += 1\n",
    "        #clump = A.pop()\n",
    "        #for i in clump:\n",
    "            #temp_labels[i] = t\n",
    "        #m += 1\n",
    "    \n",
    "    # Replace labels with -1 with '?' and reindex labels \n",
    "    t, v = -1, {}\n",
    "    Labels = []\n",
    "    for i in temp_labels:\n",
    "        if i == -1:\n",
    "            Labels += [ '?' ]\n",
    "        else:\n",
    "            if i not in v: \n",
    "                v[i] = 0\n",
    "                t += 1\n",
    "            Labels += [t]\n",
    "    \n",
    "    # assign + return\n",
    "    return Labels\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d95ec-74bd-4bf5-9471-632a11fe2816",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#########################################\n",
    "#### Measurements using hand model ######\n",
    "#########################################\n",
    "\n",
    "import sympy \n",
    "frame = 128\n",
    "annotated_image = np.zeros((1024,1024,3),dtype=np.uint8)\n",
    "landmarks = get_landmarks_data(frame)\n",
    "\n",
    "# Triangle for hand \n",
    "X = landmarks.right_hand_landmarks\n",
    "\n",
    "# z-prediction\n",
    "from sympy import *\n",
    "from sympy import sqrt\n",
    "z1, z2 = symbols('z1, z2')\n",
    "\n",
    "Measurements = {\n",
    " (0, 1) : 0.19917,\n",
    " (1, 2) : 0.18034,\n",
    " (2, 3) : 0.12843,\n",
    " (3, 4) : 0.09624,\n",
    " (0, 5) : 0.41711,\n",
    " (5, 6) : 0.16196,\n",
    " (6, 7) : 0.10277,\n",
    " (7, 8) : 0.08568,\n",
    " (0, 9) : 0.40355,\n",
    " (9, 10) : 0.18095,\n",
    " (10, 11) : 0.11845,\n",
    " (11, 12) : 0.09565,\n",
    " (0, 13) : 0.3855,\n",
    " (13, 14) : 0.16929,\n",
    " (14, 15) : 0.10963,\n",
    " (15, 16) : 0.09177,\n",
    " (0, 17) : 0.37061,\n",
    " (17, 18) : 0.13211,\n",
    " (18, 19) : 0.08178,\n",
    " (19, 20) : 0.07294 }\n",
    "\n",
    "# z-prediction - given points \n",
    "def triangle_prediction(X,Y):\n",
    "    \n",
    "    # dummy variable for z value of X,Y\n",
    "    V = [ X[0], X[1], z1 ] \n",
    "    W = [ Y[0], Y[1], z2 ]\n",
    "    \n",
    "    # Create Equations\n",
    "    E1 = Eq( np.dot(V,W), sqrt(np.dot(V,V) * np.dot(W,W)) * (0.6781246776238563) ) # dot product a * b = |a||b| cos( theta )\n",
    "    E2 = Eq( np.dot(V,V), np.dot(W,W) * (1.1254837879588218)**2 ) # Ratio\n",
    "    \n",
    "    # set determinant = 0 and solve \n",
    "    ans = list(nonlinsolve([E1,E2], [z1, z2]))\n",
    "    # print(ans)\n",
    "    \n",
    "    # new z-coordinates\n",
    "    z1_new, z2_new = float(ans[0][0]), float(ans[0][1])\n",
    "    \n",
    "    # return\n",
    "    return z1_new, z2_new \n",
    "\n",
    "\n",
    "def fix_z(X):\n",
    "    \n",
    "    # shift to origin\n",
    "    X = shift( X, np.array([0, 0, 0]) )\n",
    "    \n",
    "    ### Fundamental triangle 0--5--17\n",
    "    # Build new (0,5) and (0,17) vectors\n",
    "    X5, X17 = Landmark_vector(X, 5), Landmark_vector(X, 17) \n",
    "    X5[2], X17[2] = triangle_prediction(X5,X17)\n",
    "    # print(X5[2], X17[2])\n",
    "      \n",
    "    # initialize measurements\n",
    "    base = np.linalg.norm(X5)\n",
    "    measurement_base = Measurements[(0,5)]\n",
    "    \n",
    "    # initizalize obj\n",
    "    obj = landmark_pb2.LandmarkList()\n",
    "    obj.landmark.add( x=0, y=0, z=0 ) # set 0 landmark to origin\n",
    "    \n",
    "    # loop \n",
    "    for i,j in Measurements:\n",
    "        if (i,j) == (0,5): \n",
    "            obj.landmark.add( x=X5[0], y=X5[1], z=X5[2])\n",
    "        elif (i,j) == (0,17):\n",
    "            obj.landmark.add( x=X17[0], y=X17[1], z=X17[2] )\n",
    "        else:\n",
    "            # vector connecting i and j (use variable for z)\n",
    "            Xi, Xj = Landmark_vector(obj, i), Landmark_vector(X, j)\n",
    "            V = Xj - Xi\n",
    "            V = [V[0], V[1], z1]\n",
    "            \n",
    "            # Equation\n",
    "            E = Eq( sqrt(np.dot(V,V)) / base, Measurements[(i,j)] / measurement_base )\n",
    "\n",
    "            # solution \n",
    "            corr = list(solveset(E, z1))\n",
    "            # print( j, corr, Xj[2], Xi[2] + corr[0])\n",
    "            \n",
    "            corr = corr[0]\n",
    "                  \n",
    "            # convert to float if real, else set to 0\n",
    "            corr = float(corr) if corr.is_real else 0\n",
    "\n",
    "            # set coordinates\n",
    "            obj.landmark.add( x=Xj[0], y=Xj[1], z=Xi[2] + corr )\n",
    "    \n",
    "    return obj\n",
    "        \n",
    "\n",
    "# Recenters a hand based on the outer landmarks \n",
    "def centered(X, frame):\n",
    "    \n",
    "    # if not defined\n",
    "    if Landmark_is_nan(X): return X\n",
    "\n",
    "    # Centering\n",
    "    X0 = Landmark_vector(X,0) # base\n",
    "    X1 = Landmark_vector(X,1) # left \n",
    "    X5 = Landmark_vector(X,5) # center left \n",
    "    X9 = Landmark_vector(X,9) # center\n",
    "    X13 = Landmark_vector(X,13) # center right\n",
    "    X17 = Landmark_vector(X,17) # right \n",
    "        \n",
    "    # Possible basis vectors \n",
    "    Base = [ (1, X1-X0), (5, X5-X0), (9, X9-X0), (13, X13-X0), (17, X17-X0) ] \n",
    "    \n",
    "    # Sort basis vectors by z-coordinates\n",
    "    Base = sorted( Base, key=lambda x: x[1][2] )\n",
    "    \n",
    "    # Select vectors \n",
    "    L = [ Base[0] ] + [ sorted(Base[1:4], key=lambda x: abs(x[1][2] - Base[0][1][2]) * abs(x[1][2] - Base[4][1][2]))[2] ] + [ Base[4] ] \n",
    "    \n",
    "    print(L)\n",
    "    \n",
    "    D = { \n",
    "    1  : [ 0.06816408, -0.02943546,  0.        ], \n",
    "    5  : [ 0.04396006, -0.13104451, -0.01766506], \n",
    "    9  : [ 0.00693706, -0.12849939, -0.04018739], \n",
    "    13 : [-0.02584845, -0.11831534, -0.04380703],\n",
    "    17 : [-0.05505851, -0.10182643, -0.04391743] }\n",
    "    \n",
    "    # Basis Matrices\n",
    "    A, B = np.array([i[1] for i in L]),  np.array([ D[i[0]] for i in L ])\n",
    "    A1, B1 = np.array([ i[1] for i in Base[0:3]]),  np.array([ D[i] for i in [1,5,9]])\n",
    "    print(A1,B1)\n",
    "    \n",
    "    # Transformation\n",
    "    C = (np.linalg.inv(A)).dot(B)\n",
    "    C1 = (np.linalg.inv(A1)).dot(B1)\n",
    "    \n",
    "    print(np.linalg.det(C), np.linalg.det(C1))\n",
    "    # print( np.linalg.det(C), (np.linalg.norm(V5) / np.linalg.norm(V17) ), np.dot(V5,V17) / (np.linalg.norm(V5) * np.linalg.norm(V17))  )\n",
    "    \n",
    "    obj = landmark_pb2.LandmarkList()\n",
    "    for i in X.landmark:\n",
    "            \n",
    "            # form vector\n",
    "            v = np.array([i.x,i.y,i.z])\n",
    "            \n",
    "            # affine shift\n",
    "            v = v - X0\n",
    "            \n",
    "            # linear transformation\n",
    "            v = v.dot(C)\n",
    "            \n",
    "            # set coordinates\n",
    "            obj.landmark.add( x=v[0], y=v[1], z=v[2] )\n",
    "            \n",
    "    return obj       \n",
    "\n",
    "#########################################\n",
    "###### Entropy (letter from phrase) #####\n",
    "#########################################\n",
    "\n",
    "sphrase = phrase.replace(\" \", \"\")\n",
    "\n",
    "def letter_predict(frame):\n",
    "    i = math.floor( len(phrase) * frame / len(frames) ) \n",
    "    return phrase[i]\n",
    "\n",
    "def letter_prediction(frame):\n",
    "    i = bisect.bisect_left(T,frame)\n",
    "    if len(sphrase) <= i: i = len(sphrase)-1\n",
    "    return sphrase[i]\n",
    "\n",
    "# fingers\n",
    "hand_obj = { 'thumb' : [1,2,3,4], 'pointer' : [6,7,8], 'middle' : [10,11,12], 'ring' : [14,15,16], 'pinky' : [18,19,20], 'palm' : [0,5,9,13,17] }\n",
    "\n",
    "# measure the sum of the distances between handlandmarks in any two frames \n",
    "def entropy(frame):\n",
    "    \n",
    "    L = []\n",
    "    for frame in frames:\n",
    "    \n",
    "        # select hand landmarks from frame\n",
    "        if dominant_hand == 'right': X = center( get_landmarks_data(frame).right_hand_landmarks, frame )\n",
    "        else: X = center( get_landmarks_data(frame).left_hand_landmarks, frame )\n",
    "    \n",
    "        # check if defined\n",
    "        if Landmark_is_nan(X): continue\n",
    "        \n",
    "        Eb, Ef = {}, {}\n",
    "        for obj in hand_obj:\n",
    "        \n",
    "            # loop over landmarks and add to entropy\n",
    "            db, df = 0, 0\n",
    "            for i in hand_obj[obj]:\n",
    "            \n",
    "                # a = coordinates of i^th landmark in frame,  b = coordinate of i^th landmark in (frame - 1)\n",
    "                a = Landmark_vector(X,i) \n",
    "                b = Landmark_vector(Y,i)\n",
    "                c = Landmark_vector(Z,i) \n",
    "            \n",
    "                # remove z-values\n",
    "                a[2], b[2], c[2] = 0,0,0\n",
    "            \n",
    "                # distance \n",
    "                db += np.linalg.norm(a-b,2)\n",
    "                df += np.linalg.norm(b-c,2)\n",
    "            \n",
    "            # add to entropy\n",
    "            Eb[obj] = db\n",
    "            Ef[obj] = df\n",
    "    \n",
    "    # total entropy\n",
    "    teb = round( sum(Eb[i] for i in Eb) * 100 , 3 )\n",
    "    tef = round( sum(Ef[i] for i in Ef) * 100 , 3 )\n",
    "    \n",
    "    # total entropy\n",
    "    total_entropy = min(teb,tef)\n",
    "    \n",
    "    # print(\"heyo\", teb, tef)\n",
    "   \n",
    "    # return entropy\n",
    "    return total_entropy\n",
    "'''        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70a0f0b-6c1f-4043-ab82-f1d07a63b345",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# List all characters from sentence\n",
    "# characters = V.characters\n",
    "# frames = V.frames\n",
    "\n",
    "# Good frames\n",
    "# GoodFrames = [i for i in frames if prediction(V,i) != '?']\n",
    "\n",
    "# training data\n",
    "# train_data = [ V.normal_hand_data + V.hand_position for i in GoodFrames]\n",
    "# train_predictions = [ Encode[prediction(V,i)] for i in GoodFrames ]\n",
    "train_x, train_y = pickles\n",
    "train_y = [ Encode[i] for i in train_y ]\n",
    "\n",
    "# Model\n",
    "k = len( Characters )\n",
    "M = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(20, 3)),\n",
    "    tf.keras.layers.Dense(6*k, activation='relu'),\n",
    "    tf.keras.layers.Dense(k)])\n",
    "\n",
    "# Compile Model\n",
    "M.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "M.fit( train_x , train_y, epochs = 100, verbose=0 )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69c4995-bb5f-49bd-9c92-33b9c270af65",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kaggle dataset\n",
    "# https://www.kaggle.com/datasets/grassknoted/asl-alphabet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

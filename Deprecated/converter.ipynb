{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1e4b9df-1bf1-4dfa-b6b5-eddc23a1ecb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from landmarks.ipynb\n",
      "importing Jupyter notebook from decode_phrase.ipynb\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "import cv2\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import ipywidgets as widgets\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from mediapipe import solutions\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from fastai.vision.all import show_image\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import import_ipynb\n",
    "from landmarks import *\n",
    "from decode_phrase import *\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose\n",
    "# mp_holistic = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86c663aa-80c5-4ed6-ad08-119358c1c22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# mediapipe hand detector (make sure hand_landmarker.task is in folder)\n",
    "base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "options = vision.HandLandmarkerOptions(base_options=base_options, min_hand_detection_confidence=.5, num_hands=2)\n",
    "detector = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "#BaseOptions = mp.tasks.BaseOptions\n",
    "#HandLandmarker = mp.tasks.vision.HandLandmarker\n",
    "#HandLandmarkerOptions = mp.tasks.vision.HandLandmarkerOptions\n",
    "#VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "# Create a hand landmarker instance with the video mode:\n",
    "#options = HandLandmarkerOptions( base_options=BaseOptions(model_asset_path='hand_landmarker.task'), running_mode=VisionRunningMode.VIDEO)\n",
    "#detector = HandLandmarker.create_from_options(options)\n",
    "\n",
    "def convert_to_mediapipe(path):\n",
    "    \n",
    "    ### initialize ###\n",
    "    # open video \n",
    "    vid = cv2.VideoCapture(path)\n",
    "    assert vid.isOpened(), 'Make sure that the video is in a format accepted by c2v.VideoCapture()' \n",
    "    \n",
    "    # load first frame \n",
    "    boo, image = vid.read()\n",
    "    assert boo, 'Unable to load first frame of video'\n",
    "    \n",
    "    ### loop over frames and apply mediapipe ### \n",
    "    count = 1\n",
    "    images, landmarks = [], []\n",
    "    while boo:\n",
    "        \n",
    "        # convert image to mediapipe\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # change color from Blue,Green,Red to Red,Green,Blue\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "        \n",
    "        # use mediapipe to finc image\n",
    "        result = detector.detect(mp_image)\n",
    "        \n",
    "        # cv2.imwrite(\"image\"+str(count)+\".jpeg\", image)\n",
    "        # add to lists \n",
    "        images += [image ]\n",
    "        landmarks += [ result ]\n",
    "        \n",
    "        # update \n",
    "        boo, image = vid.read()\n",
    "        count += 1\n",
    "        \n",
    "    return images, landmarks \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c4cd481-408c-4bd0-a8db-9fd132eb0701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: current only has right hand\n",
    "def process_video(self):\n",
    "   \n",
    "    L = []\n",
    "    for frame in self.landmarks:\n",
    "        if frame.hand_landmarks:\n",
    "            \n",
    "            # create the landmark\n",
    "            x = landmark_pb2.LandmarkList()\n",
    "            for v in frame.hand_landmarks[0]:\n",
    "                x.landmark.add( x=v.x, y=v.y, z=v.z ) \n",
    "            \n",
    "            # center the landmark\n",
    "            x = center(x)\n",
    "            \n",
    "            # extract the data \n",
    "            data = np.array([ [ Landmark_vector(x,i) for i in range(1,21) ] ])\n",
    "            \n",
    "            # add to L\n",
    "            L += [ data ]\n",
    "        else:\n",
    "            L += [ np.array([ [ np.array([0.,0.,0.]) for i in range(20) ] ]) ]   \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d671dcb-0712-4664-9086-b676aade7ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "##### Video class #####\n",
    "#######################\n",
    "                \n",
    "### Video file Class\n",
    "# set label = false if you do not want to automatically label the video\n",
    "class video_file:           \n",
    "    def __init__(self, path, label = True):\n",
    "        \n",
    "        # initialize\n",
    "        frames, landmarkers = convert_to_mediapipe(path)\n",
    "        \n",
    "        # attributes\n",
    "        self.path = path\n",
    "        self.frames = frames \n",
    "        self.landmarks = landmarkers\n",
    "        self.total_frames = len(frames)\n",
    "        self.total_hand_frames = len([ x for x in self.landmarks if x.hand_landmarks])\n",
    "        self.landmark_percentage = 0 if self.total_frames == 0 else self.total_hand_frames / self.total_frames\n",
    "        \n",
    "        # label video\n",
    "        if label:\n",
    "            self.hand_frames = process_video(self)\n",
    "            phrase, labels = Corg(self.hand_frames)\n",
    "            self.phrase = phrase\n",
    "            self.labels = labels\n",
    "            \n",
    "\n",
    "    def __repr__(self): \n",
    "            a = \" path: {0}\\n Number of frames: {1}\\n Percentage of frames with landmarks: {2}\".format( self.path,\n",
    "                                                                                                        self.total_frames, \n",
    "                                                                                                        self.landmark_percentage ) \n",
    "\n",
    "            return a \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ff71eee-e5fe-4cb8-880b-d6e165b87b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MARGIN = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "    hand_landmarks_list = detection_result.hand_landmarks\n",
    "    handedness_list = detection_result.handedness\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "\n",
    "    # Loop through the detected hands to visualize.\n",
    "    for idx in range(len(hand_landmarks_list)):\n",
    "        hand_landmarks = hand_landmarks_list[idx]\n",
    "        handedness = handedness_list[idx]\n",
    "\n",
    "        # Draw the hand landmarks.\n",
    "        hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        hand_landmarks_proto.landmark.extend([\n",
    "        landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks])\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "                                                annotated_image,\n",
    "                                                hand_landmarks_proto,\n",
    "                                                solutions.hands.HAND_CONNECTIONS,\n",
    "                                                solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "                                                solutions.drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        # Get the top left corner of the detected hand's bounding box.\n",
    "        height, width, _ = annotated_image.shape\n",
    "        x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
    "        y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
    "        text_x = int(min(x_coordinates) * width)\n",
    "        text_y = int(min(y_coordinates) * height) - MARGIN\n",
    "\n",
    "        # Draw handedness (left or right hand) on the image.\n",
    "        cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n",
    "                (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
    "                FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
    "\n",
    "    return annotated_image\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6ba27d4-9222-4d8a-b449-81a55e05462a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_annotated_video(self):\n",
    "    \n",
    "    data = self.frames\n",
    "    land = self.landmarks\n",
    "    labels = self.labels\n",
    "    phrase = self.phrase\n",
    "    \n",
    "    # show function\n",
    "    def show_frame(i):\n",
    "        \n",
    "        # annotated image and show\n",
    "        annotated_image = draw_landmarks_on_image(data[i], land[i])\n",
    "        show_image(annotated_image, figsize=(6,6), title=f'Frame: {i} of {len(data)}   Phrase: {phrase}    Prediction: {labels[i]}')\n",
    "    \n",
    "    return show_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df9bb450-6bfe-4949-b2ef-e9a98ad21df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 107, 47), (107, 166, 40), (166, 233, 32), (233, 316, 49), (316, 399, 38), (399, 464, 40), (464, 519, 50)]\n",
      " path: Samples/IMG_0958.MOV\n",
      " Number of frames: 520\n",
      " Percentage of frames with landmarks: 1.0\n"
     ]
    }
   ],
   "source": [
    "path = 'Samples/IMG_0958.MOV'\n",
    "V = video_file(path)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ce4f8d04-0bb0-4a04-b09b-7aff94700298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccccba3fdc4c4a2f9529f160ae557961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='i', layout=Layout(width='1000px'), max=519), Output()), â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.show_annotated_video.<locals>.show_frame(i)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = show_annotated_video(V)\n",
    "interact(f, i=widgets.IntSlider(min=0, max=len(V.frames)-1, step=1, value=0, layout=widgets.Layout(width='1000px')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d849e81e-7bb8-48fe-ba97-5422fad7d609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = '/Users/benjaminbreen/Desktop/test_hand.jpg'\n",
    "# model_path = \n",
    "\n",
    "# Load the input image from an image file.\n",
    "# mp_image = mp.Image.create_from_file(model_path)\n",
    "\n",
    "# create img\n",
    "# img = cv2.imread(model_path)\n",
    "\n",
    "# STEP 2: Create an HandLandmarker object.\n",
    "#base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "#options = vision.HandLandmarkerOptions(base_options=base_options, num_hands=2)\n",
    "#detector = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "# STEP 3: Load the input image.\n",
    "# image = mp.Image.create_from_file(model_path)\n",
    "#mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "\n",
    "# STEP 4: Detect hand landmarks from the input image.\n",
    "#detection_result = detector.detect(mp_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "09826650-066d-48f9-a92c-2ae8f3df7397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from spellchecker import SpellChecker\n",
    "from Levenshtein import distance\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "# Labeling\n",
    "\n",
    "# Encode\n",
    "E = {\" \":0, \"!\":1, \"#\":2, \"$\":3, \"%\":4, \"&\":5, \"'\":6, \"(\":7, \")\":8, \"*\":9, \"+\":10, \",\":11, \"-\":12, \".\":13, \n",
    "          \"/\":14, \"0\":15, \"1\":16, \"2\":17, \"3\":18, \"4\":19, \"5\":20, \"6\":21, \"7\":22, \"8\":23, \"9\":24, \":\":25, \";\":26,\n",
    "          \"=\":27, \"?\":28, \"@\":29, \"[\":30, \"_\":31, \"a\":32, \"b\":33, \"c\":34, \"d\":35, \"e\":36, \"f\":37, \"g\":38, \"h\":39,\n",
    "          \"i\":40, \"j\":41, \"k\":42, \"l\":43, \"m\":44, \"n\":45, \"o\":46, \"p\":47, \"q\":48, \"r\":49, \"s\":50, \"t\":51, \"u\":52, \n",
    "          \"v\":53, \"w\":54, \"x\":55, \"y\":56, \"z\":57, \"~\":58}\n",
    "# Decode\n",
    "D = {j:i for i,j in E.items()}\n",
    "\n",
    "# Encode - converts letter to number\n",
    "def Encode(x):\n",
    "    return E[x]\n",
    "\n",
    "# Decode - converts number to letter\n",
    "def Decode(x):\n",
    "    return D[x]\n",
    "\n",
    "def Corg(frames):\n",
    "    \n",
    "    # initialize \n",
    "    m = len(frames)\n",
    "    thresh = .1 #threshold\n",
    "    n = 59\n",
    "    \n",
    "    # load model \n",
    "    M = tf.keras.models.load_model('alphabetmodel.h5')\n",
    "    X = tf.keras.Sequential([ M, tf.keras.layers.Softmax() ]) # Need softmax later for predictions\n",
    "    \n",
    "    # Predictions\n",
    "    Predictions = []\n",
    "    for x in frames:\n",
    "        if np.any(x):\n",
    "            Predictions.append( X.predict(np.array(x), verbose=0)[0] )\n",
    "        else:\n",
    "            Predictions.append( np.array( [0.] * 59 )  )\n",
    "    \n",
    "    ## score function \n",
    "    # Given a letter (e.g. 'a') and cluster label c (e.g. 0) returns a score of the letter for that clump \n",
    "    def score( i, letter ):\n",
    "        s = Predictions[i][letter]\n",
    "        return s\n",
    "    \n",
    "\n",
    "    ### Prefix sums\n",
    "    D = {}\n",
    "    for c in range(59):\n",
    "        \n",
    "        # initialize\n",
    "        D[c] = defaultdict(int)\n",
    "        pre, L = 0, []\n",
    "        \n",
    "        # loop\n",
    "        for j in range(m):\n",
    "            \n",
    "            # update prefix sum\n",
    "            pre += score(j,c)\n",
    "            \n",
    "            # loop over past prefix sum and compute score of i,j\n",
    "            for i,x in enumerate(L):\n",
    "                D[c][(i,j)] = (pre - x) * math.sqrt(j-i)\n",
    "            \n",
    "            # update list of prefixes\n",
    "            L += [pre]\n",
    "    \n",
    "    ### Dynamic programming\n",
    "    # dp array\n",
    "    dp = defaultdict( lambda : (0,[]) )\n",
    "    for i in range(m):\n",
    "        \n",
    "        if not np.any(Predictions[i]):\n",
    "            continue\n",
    "        \n",
    "        # update past\n",
    "        if dp[i][0] < dp[i-1][0]:\n",
    "            dp[i] = dp[i-1]\n",
    "        \n",
    "        # update future\n",
    "        for j in range(i+2, m):\n",
    "            \n",
    "            # loop\n",
    "            for c in range(59):\n",
    "                c_score = D[c][(i+1,j)]\n",
    "                if dp[j][0] < c_score + dp[i][0]:\n",
    "                     dp[j] = ( c_score + dp[i][0], dp[i][1] + [(i,j,c)] )\n",
    "    \n",
    "    # optimal score\n",
    "    score, labs = dp[m-1]\n",
    "    \n",
    "    # new labels\n",
    "    newlabels = ['?'] * m\n",
    "    for i, j, letter in labs:\n",
    "        for k in range(i,j+1):\n",
    "            newlabels[k] = Decode(letter)\n",
    "    \n",
    "    # self.data_labels = newlabels\n",
    "    print(labs)\n",
    "            \n",
    "    # sentence\n",
    "    s = ''.join( Decode(i[2]) for i in labs)\n",
    "    \n",
    "    # return s, labs, D, [ Decode[np.argmax(i)] for i in Predictions]\n",
    "    return s, newlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "54225aad-1982-4170-8da9-7b68f4d47ab0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spell' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mreconstruct_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpiargis\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:133\u001b[0m, in \u001b[0;36mreconstruct_sentence\u001b[0;34m(s)\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spell' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe0e762-5017-4d19-86b5-459c90e8f561",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
